{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Fraud Detection in Python**\n",
    "A typical organization loses an estimated 5% of its yearly revenue to fraud. In this course, learn to fight fraud by using data. Apply supervised learning algorithms to detect fraudulent behavior based upon past fraud, and use unsupervised learning methods to discover new types of fraud activities. \n",
    "\n",
    "Fraudulent transactions are rare compared to the norm.  As such, learn to properly classify imbalanced datasets.\n",
    "\n",
    "This notebook technical and theoretical insights and demonstrates how to implement fraud detection models. Finally, get tips and advice from real-life experience to help prevent common mistakes in fraud analytics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from pprint import pprint as pp\n",
    "import csv\n",
    "from pathlib import Path\n",
    "import string\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    r2_score,\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    accuracy_score,\n",
    "    roc_auc_score,\n",
    "    roc_curve,\n",
    "    precision_recall_curve,\n",
    "    average_precision_score,\n",
    ")\n",
    "\n",
    "import gensim\n",
    "from gensim import corpora"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pandas Configuration Options**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_columns\", 700)\n",
    "pd.set_option(\"display.max_rows\", 400)\n",
    "pd.set_option(\"display.min_rows\", 10)\n",
    "pd.set_option(\"display.expand_frame_repr\", True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data Files Location**\n",
    "\n",
    "* Most data files for the exercises can be found on the [this site](https://www.datacamp.com/courses/fraud-detection-in-python):\n",
    "    * [Chapter 4](https://assets.datacamp.com/production/repositories/2162/datasets/94f2356652dc9ea8f0654b5e9c29645115b6e77f/chapter_4.zip)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data File Objects**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = Path.cwd() / \"data\"\n",
    "\n",
    "ch4 = data / \"chapter_4\"\n",
    "enron_emails_clean_file = ch4 / \"enron_emails_clean.csv\"\n",
    "cleantext_file = ch4 / \"cleantext.pickle\"\n",
    "corpus_file = ch4 / \"corpus.pickle\"\n",
    "dict_file = ch4 / \"dict.pickle\"\n",
    "ldamodel_file = ch4 / \"ldamodel.pickle\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to fraud detection\n",
    "\n",
    "* Types:\n",
    "    * Insurance\n",
    "    * Credit card\n",
    "    * Identity theft\n",
    "    * Money laundering\n",
    "    * Tax evasion\n",
    "    * Healthcare\n",
    "    * Product warranty\n",
    "* e-commerce businesses must continuously assess the legitimacy of client transactions\n",
    "* Detecting fraud is challenging:\n",
    "    * Uncommon; < 0.01% of transactions\n",
    "    * Attempts are made to conceal fraud\n",
    "    * Behavior evolves\n",
    "    * Fraudulent activities perpetrated by networks - organized crime\n",
    "* Fraud detection requires training an algorithm to identify concealed observations from any normal observations\n",
    "* Fraud analytics teams:\n",
    "    * Often use rules based systems, based on manually set thresholds and experience\n",
    "    * Check the news\n",
    "    * Receive external lists of fraudulent accounts and names\n",
    "        * suspicious names or track an external hit list from police to reference check against the client base\n",
    "    * Sometimes use machine learning algorithms to detect fraud or suspicious behavior\n",
    "        * Existing sources can be used as inputs into the ML model\n",
    "        * Verify the veracity of rules based labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fraud detection using text\n",
    "\n",
    "Use text data, text mining and topic modeling to detect fraudulent behavior."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using text data\n",
    "\n",
    "* Types of useful text data:\n",
    "    1. Emails from employees and/or clients\n",
    "    1. Transaction descriptions\n",
    "    1. Employee notes\n",
    "    1. Insurance claim form description box\n",
    "    1. Recorded telephone conversations\n",
    "* Text mining techniques for fraud detection\n",
    "    1. Word search\n",
    "    1. Sentiment analysis\n",
    "    1. Word frequencies and topic analysis\n",
    "    1. Style\n",
    "* Word search for fraud detection\n",
    "    * Flagging suspicious words:\n",
    "        1. Simple, straightforward and easy to explain\n",
    "        1. Match results can be used as a filter on top of machine learning model\n",
    "        1. Match results can be used as a feature in a machine learning model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word counts to flag fraud with pandas\n",
    "\n",
    "```python\n",
    "# Using a string operator to find words\n",
    "df['email_body'].str.contains('money laundering')\n",
    "\n",
    " # Select data that matches \n",
    "df.loc[df['email_body'].str.contains('money laundering', na=False)]\n",
    "\n",
    " # Create a list of words to search for\n",
    "list_of_words = ['police', 'money laundering']\n",
    "df.loc[df['email_body'].str.contains('|'.join(list_of_words), na=False)]\n",
    "\n",
    " # Create a fraud flag \n",
    "df['flag'] = np.where((df['email_body'].str.contains('|'.join(list_of_words)) == True), 1, 0)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word search with dataframes\n",
    "\n",
    "In this section, you're going to work with text data, containing emails from Enron employees. The **Enron scandal** is a famous fraud case. Enron employees covered up the bad financial position of the company, thereby keeping the stock price artificially high. Enron employees sold their own stock options, and when the truth came out, Enron investors were left with nothing. The goal is to find all emails that mention specific words, such as \"sell enron stock\".\n",
    "\n",
    "By using string operations on dataframes, you can easily sift through messy email data and create flags based on word-hits. The Enron email data has been put into a dataframe called `df` so let's search for suspicious terms. Feel free to explore `df` in the Console before getting started.\n",
    "\n",
    "**Instructions 1/2**\n",
    "\n",
    "* Check the head of `df` in the console and look for any emails mentioning 'sell enron stock'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(enron_emails_clean_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Message-ID</th>\n",
       "      <th>From</th>\n",
       "      <th>To</th>\n",
       "      <th>Date</th>\n",
       "      <th>content</th>\n",
       "      <th>clean_content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;8345058.1075840404046.JavaMail.evans@thyme&gt;</td>\n",
       "      <td>('advdfeedback@investools.com')</td>\n",
       "      <td>('advdfeedback@investools.com')</td>\n",
       "      <td>2002-01-29 23:20:55</td>\n",
       "      <td>INVESTools Advisory\\nA Free Digest of Trusted ...</td>\n",
       "      <td>investools advisory free digest trusted invest...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&lt;1512159.1075863666797.JavaMail.evans@thyme&gt;</td>\n",
       "      <td>('richard.sanders@enron.com')</td>\n",
       "      <td>('richard.sanders@enron.com')</td>\n",
       "      <td>2000-09-20 19:07:00</td>\n",
       "      <td>----- Forwarded by Richard B Sanders/HOU/ECT o...</td>\n",
       "      <td>forwarded richard b sanders hou ect pm justin ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>&lt;26118676.1075862176383.JavaMail.evans@thyme&gt;</td>\n",
       "      <td>('m..love@enron.com')</td>\n",
       "      <td>('m..love@enron.com')</td>\n",
       "      <td>2001-10-30 16:15:17</td>\n",
       "      <td>hey you are not wearing your target purple shi...</td>\n",
       "      <td>hey wearing target purple shirt today mine wan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>&lt;10369289.1075860831062.JavaMail.evans@thyme&gt;</td>\n",
       "      <td>('leslie.milosevich@kp.org')</td>\n",
       "      <td>('leslie.milosevich@kp.org')</td>\n",
       "      <td>2002-01-30 17:54:18</td>\n",
       "      <td>Leslie Milosevich\\n1042 Santa Clara Avenue\\nAl...</td>\n",
       "      <td>leslie milosevich santa clara avenue alameda c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>&lt;26728895.1075860815046.JavaMail.evans@thyme&gt;</td>\n",
       "      <td>('rtwait@graphicaljazz.com')</td>\n",
       "      <td>('rtwait@graphicaljazz.com')</td>\n",
       "      <td>2002-01-30 19:36:01</td>\n",
       "      <td>Rini Twait\\n1010 E 5th Ave\\nLongmont, CO 80501...</td>\n",
       "      <td>rini twait e th ave longmont co rtwait graphic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2085</th>\n",
       "      <td>&lt;19039088.1075851547721.JavaMail.evans@thyme&gt;</td>\n",
       "      <td>('andy.zipper@enron.com')</td>\n",
       "      <td>('andy.zipper@enron.com')</td>\n",
       "      <td>2001-10-22 14:00:17</td>\n",
       "      <td>i bot 1,000/d at 3.175 apr/oct02. put it again...</td>\n",
       "      <td>bot apr oct put digital gas x thanks</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2086</th>\n",
       "      <td>&lt;6813352.1075842016977.JavaMail.evans@thyme&gt;</td>\n",
       "      <td>('andy.zipper@enron.com')</td>\n",
       "      <td>('andy.zipper@enron.com')</td>\n",
       "      <td>2002-01-25 17:39:38</td>\n",
       "      <td>I'm okay. How are you ?</td>\n",
       "      <td>okay</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2087</th>\n",
       "      <td>&lt;4833106.1075842022184.JavaMail.evans@thyme&gt;</td>\n",
       "      <td>('tradersummary@syncrasy.com')</td>\n",
       "      <td>('tradersummary@syncrasy.com')</td>\n",
       "      <td>2002-02-01 16:15:17</td>\n",
       "      <td>\\n[IMAGE]=09\\n\\n\\n[IMAGE] [IMAGE][IMAGE][IMAGE...</td>\n",
       "      <td>image image image image image image image imag...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2088</th>\n",
       "      <td>&lt;3550151.1075842023814.JavaMail.evans@thyme&gt;</td>\n",
       "      <td>('lmrig@qwest.net')</td>\n",
       "      <td>('lmrig@qwest.net')</td>\n",
       "      <td>2002-01-29 02:01:00</td>\n",
       "      <td>\\n\\nTransmission Expansion and Systems in Tran...</td>\n",
       "      <td>transmission expansion systems transition conf...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2089</th>\n",
       "      <td>&lt;33102456.1075842033860.JavaMail.evans@thyme&gt;</td>\n",
       "      <td>('john.zufferli@enron.com')</td>\n",
       "      <td>('john.zufferli@enron.com')</td>\n",
       "      <td>2001-08-09 18:59:27</td>\n",
       "      <td>Sorry Nella, I replied a while ago directly to...</td>\n",
       "      <td>sorry nella replied ago directly rob milnthorp...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2090 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         Message-ID  \\\n",
       "0      <8345058.1075840404046.JavaMail.evans@thyme>   \n",
       "1      <1512159.1075863666797.JavaMail.evans@thyme>   \n",
       "2     <26118676.1075862176383.JavaMail.evans@thyme>   \n",
       "3     <10369289.1075860831062.JavaMail.evans@thyme>   \n",
       "4     <26728895.1075860815046.JavaMail.evans@thyme>   \n",
       "...                                             ...   \n",
       "2085  <19039088.1075851547721.JavaMail.evans@thyme>   \n",
       "2086   <6813352.1075842016977.JavaMail.evans@thyme>   \n",
       "2087   <4833106.1075842022184.JavaMail.evans@thyme>   \n",
       "2088   <3550151.1075842023814.JavaMail.evans@thyme>   \n",
       "2089  <33102456.1075842033860.JavaMail.evans@thyme>   \n",
       "\n",
       "                                 From                               To  \\\n",
       "0     ('advdfeedback@investools.com')  ('advdfeedback@investools.com')   \n",
       "1       ('richard.sanders@enron.com')    ('richard.sanders@enron.com')   \n",
       "2               ('m..love@enron.com')            ('m..love@enron.com')   \n",
       "3        ('leslie.milosevich@kp.org')     ('leslie.milosevich@kp.org')   \n",
       "4        ('rtwait@graphicaljazz.com')     ('rtwait@graphicaljazz.com')   \n",
       "...                               ...                              ...   \n",
       "2085        ('andy.zipper@enron.com')        ('andy.zipper@enron.com')   \n",
       "2086        ('andy.zipper@enron.com')        ('andy.zipper@enron.com')   \n",
       "2087   ('tradersummary@syncrasy.com')   ('tradersummary@syncrasy.com')   \n",
       "2088              ('lmrig@qwest.net')              ('lmrig@qwest.net')   \n",
       "2089      ('john.zufferli@enron.com')      ('john.zufferli@enron.com')   \n",
       "\n",
       "                     Date                                            content  \\\n",
       "0     2002-01-29 23:20:55  INVESTools Advisory\\nA Free Digest of Trusted ...   \n",
       "1     2000-09-20 19:07:00  ----- Forwarded by Richard B Sanders/HOU/ECT o...   \n",
       "2     2001-10-30 16:15:17  hey you are not wearing your target purple shi...   \n",
       "3     2002-01-30 17:54:18  Leslie Milosevich\\n1042 Santa Clara Avenue\\nAl...   \n",
       "4     2002-01-30 19:36:01  Rini Twait\\n1010 E 5th Ave\\nLongmont, CO 80501...   \n",
       "...                   ...                                                ...   \n",
       "2085  2001-10-22 14:00:17  i bot 1,000/d at 3.175 apr/oct02. put it again...   \n",
       "2086  2002-01-25 17:39:38                            I'm okay. How are you ?   \n",
       "2087  2002-02-01 16:15:17  \\n[IMAGE]=09\\n\\n\\n[IMAGE] [IMAGE][IMAGE][IMAGE...   \n",
       "2088  2002-01-29 02:01:00  \\n\\nTransmission Expansion and Systems in Tran...   \n",
       "2089  2001-08-09 18:59:27  Sorry Nella, I replied a while ago directly to...   \n",
       "\n",
       "                                          clean_content  \n",
       "0     investools advisory free digest trusted invest...  \n",
       "1     forwarded richard b sanders hou ect pm justin ...  \n",
       "2     hey wearing target purple shirt today mine wan...  \n",
       "3     leslie milosevich santa clara avenue alameda c...  \n",
       "4     rini twait e th ave longmont co rtwait graphic...  \n",
       "...                                                 ...  \n",
       "2085               bot apr oct put digital gas x thanks  \n",
       "2086                                               okay  \n",
       "2087  image image image image image image image imag...  \n",
       "2088  transmission expansion systems transition conf...  \n",
       "2089  sorry nella replied ago directly rob milnthorp...  \n",
       "\n",
       "[2090 rows x 6 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = df[\"clean_content\"].str.contains(\"sell enron stock\", na=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Instructions 2/2**\n",
    "\n",
    "* Locate the data in `df` that meets the condition we created earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Message-ID</th>\n",
       "      <th>From</th>\n",
       "      <th>To</th>\n",
       "      <th>Date</th>\n",
       "      <th>content</th>\n",
       "      <th>clean_content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>&lt;6336501.1075841154311.JavaMail.evans@thyme&gt;</td>\n",
       "      <td>('sarah.palmer@enron.com')</td>\n",
       "      <td>('sarah.palmer@enron.com')</td>\n",
       "      <td>2002-02-01 14:53:35</td>\n",
       "      <td>\\nJoint Venture: A 1997 Enron Meeting Belies O...</td>\n",
       "      <td>joint venture enron meeting belies officers cl...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       Message-ID                        From  \\\n",
       "154  <6336501.1075841154311.JavaMail.evans@thyme>  ('sarah.palmer@enron.com')   \n",
       "\n",
       "                             To                 Date  \\\n",
       "154  ('sarah.palmer@enron.com')  2002-02-01 14:53:35   \n",
       "\n",
       "                                               content  \\\n",
       "154  \\nJoint Venture: A 1997 Enron Meeting Belies O...   \n",
       "\n",
       "                                         clean_content  \n",
       "154  joint venture enron meeting belies officers cl...  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Select the data from df using the mask\n",
    "df[mask]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**You see that searching for particular string values in a dataframe can be relatively easy, and allows you to include textual data into your model or analysis. You can use this word search as an additional flag, or as a feature in your fraud detection model. Let's look at how to filter the data using multiple search terms.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using list of terms\n",
    "\n",
    "Oftentimes you don't want to search on just one term. You probably can create a full **\"fraud dictionary\"** of terms that could potentially **flag fraudulent clients** and/or transactions. Fraud analysts often will have an idea what should be in such a dictionary. In this section, you're going to **flag a multitude of terms**, and in the next section you'll create a new flag variable out of it. The 'flag' can be used either directly in a machine learning model as a feature, or as an additional filter on top of your machine learning model results. Let's first use a list of terms to filter our data on. The dataframe containing the cleaned emails is again available as `df`.\n",
    "\n",
    "**Instructions**\n",
    "\n",
    "* Create a list to search for including 'enron stock', 'sell stock', 'stock bonus', and 'sell enron stock'.\n",
    "* Join the string terms in the search conditions.\n",
    "* Filter data using the emails that match with the list defined under `searchfor`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Message-ID</th>\n",
       "      <th>From</th>\n",
       "      <th>To</th>\n",
       "      <th>Date</th>\n",
       "      <th>content</th>\n",
       "      <th>clean_content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;8345058.1075840404046.JavaMail.evans@thyme&gt;</td>\n",
       "      <td>('advdfeedback@investools.com')</td>\n",
       "      <td>('advdfeedback@investools.com')</td>\n",
       "      <td>2002-01-29 23:20:55</td>\n",
       "      <td>INVESTools Advisory\\nA Free Digest of Trusted ...</td>\n",
       "      <td>investools advisory free digest trusted invest...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&lt;1512159.1075863666797.JavaMail.evans@thyme&gt;</td>\n",
       "      <td>('richard.sanders@enron.com')</td>\n",
       "      <td>('richard.sanders@enron.com')</td>\n",
       "      <td>2000-09-20 19:07:00</td>\n",
       "      <td>----- Forwarded by Richard B Sanders/HOU/ECT o...</td>\n",
       "      <td>forwarded richard b sanders hou ect pm justin ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>&lt;26118676.1075862176383.JavaMail.evans@thyme&gt;</td>\n",
       "      <td>('m..love@enron.com')</td>\n",
       "      <td>('m..love@enron.com')</td>\n",
       "      <td>2001-10-30 16:15:17</td>\n",
       "      <td>hey you are not wearing your target purple shi...</td>\n",
       "      <td>hey wearing target purple shirt today mine wan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>&lt;10369289.1075860831062.JavaMail.evans@thyme&gt;</td>\n",
       "      <td>('leslie.milosevich@kp.org')</td>\n",
       "      <td>('leslie.milosevich@kp.org')</td>\n",
       "      <td>2002-01-30 17:54:18</td>\n",
       "      <td>Leslie Milosevich\\n1042 Santa Clara Avenue\\nAl...</td>\n",
       "      <td>leslie milosevich santa clara avenue alameda c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>&lt;26728895.1075860815046.JavaMail.evans@thyme&gt;</td>\n",
       "      <td>('rtwait@graphicaljazz.com')</td>\n",
       "      <td>('rtwait@graphicaljazz.com')</td>\n",
       "      <td>2002-01-30 19:36:01</td>\n",
       "      <td>Rini Twait\\n1010 E 5th Ave\\nLongmont, CO 80501...</td>\n",
       "      <td>rini twait e th ave longmont co rtwait graphic...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      Message-ID  \\\n",
       "0   <8345058.1075840404046.JavaMail.evans@thyme>   \n",
       "1   <1512159.1075863666797.JavaMail.evans@thyme>   \n",
       "2  <26118676.1075862176383.JavaMail.evans@thyme>   \n",
       "3  <10369289.1075860831062.JavaMail.evans@thyme>   \n",
       "4  <26728895.1075860815046.JavaMail.evans@thyme>   \n",
       "\n",
       "                              From                               To  \\\n",
       "0  ('advdfeedback@investools.com')  ('advdfeedback@investools.com')   \n",
       "1    ('richard.sanders@enron.com')    ('richard.sanders@enron.com')   \n",
       "2            ('m..love@enron.com')            ('m..love@enron.com')   \n",
       "3     ('leslie.milosevich@kp.org')     ('leslie.milosevich@kp.org')   \n",
       "4     ('rtwait@graphicaljazz.com')     ('rtwait@graphicaljazz.com')   \n",
       "\n",
       "                  Date                                            content  \\\n",
       "0  2002-01-29 23:20:55  INVESTools Advisory\\nA Free Digest of Trusted ...   \n",
       "1  2000-09-20 19:07:00  ----- Forwarded by Richard B Sanders/HOU/ECT o...   \n",
       "2  2001-10-30 16:15:17  hey you are not wearing your target purple shi...   \n",
       "3  2002-01-30 17:54:18  Leslie Milosevich\\n1042 Santa Clara Avenue\\nAl...   \n",
       "4  2002-01-30 19:36:01  Rini Twait\\n1010 E 5th Ave\\nLongmont, CO 80501...   \n",
       "\n",
       "                                       clean_content  \n",
       "0  investools advisory free digest trusted invest...  \n",
       "1  forwarded richard b sanders hou ect pm justin ...  \n",
       "2  hey wearing target purple shirt today mine wan...  \n",
       "3  leslie milosevich santa clara avenue alameda c...  \n",
       "4  rini twait e th ave longmont co rtwait graphic...  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a list of terms to search for\n",
    "searchfor = [\"enron stock\", \"sell stock\", \"stock bonus\", \"sell enron stock\"]\n",
    "\n",
    "# Filter cleaned emails on searchfor list and select from df\n",
    "filtered_emails = df[df.clean_content.str.contains(\"|\".join(searchfor), na=False)]\n",
    "filtered_emails.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**By joining the search terms with the 'or' sign, i.e. |, you can search on a multitude of terms in your dataset very easily. Let's now create a flag from this which you can use as a feature in a machine learning model.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a flag\n",
    "\n",
    "This time you are going to **create an actual flag** variable that gives a **1 when the emails get a hit** on the search terms of interest, and 0 otherwise. This is the last step you need to make in order to actually use the text data content as a feature in a machine learning model, or as an actual flag on top of model results. You can continue working with the dataframe `df` containing the emails, and the `searchfor` list is the one defined in the last section.\n",
    "\n",
    "**Instructions**\n",
    "\n",
    "* Use a numpy where condition to flag '1' where the cleaned email contains words on the `searchfor` list and 0 otherwise.\n",
    "* Join the words on the `searchfor` list with an \"or\" indicator.\n",
    "* Count the values of the newly created flag variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    1776\n",
      "1     314\n",
      "Name: flag, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Create flag variable where the emails match the searchfor terms\n",
    "df[\"flag\"] = np.where(\n",
    "    (df[\"clean_content\"].str.contains(\"|\".join(searchfor)) == True), 1, 0\n",
    ")\n",
    "\n",
    "# Count the values of the flag variable\n",
    "count = df[\"flag\"].value_counts()\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**You have now managed to search for a list of strings in several lines of text data. These skills come in handy when you want to flag certain words based on what you discovered in your topic model, or when you know beforehand what you want to search for. In the next sections you're going to learn how to clean text data and to create your own topic model to further look for indications of fraud in your text data.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text mining to detect fraud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cleaning your text data\n",
    "\n",
    "**Must dos when working with textual data:**\n",
    "\n",
    "1. Tokenization\n",
    "    * Split the text into sentences and the sentences in words\n",
    "    * Transform everything to lowercase\n",
    "    * Remove punctuation\n",
    "1. Remove all stopwords\n",
    "1. Lemmatize \n",
    "    * Change from third person into first person\n",
    "    * Change past and future tense verbs to present tense\n",
    "    * This makes it possible to combine all words that point to the same thing\n",
    "1. Stem the words\n",
    "    * Reduce words to their root form\n",
    "    * e.g. walking and walked to walk\n",
    "\n",
    "* **Unprocessed Text**\n",
    "    * ![](https://raw.githubusercontent.com/trenton3983/DataCamp/master/Images/fraud_detection/text_df.JPG)\n",
    "* **Processed Text**\n",
    "    * ![](https://raw.githubusercontent.com/trenton3983/DataCamp/master/Images/fraud_detection/text_processed.JPG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Preprocessing I\n",
    "\n",
    "* Tokenizers divide strings into list of substrings\n",
    "* nltk word tokenizer can be used to find the words and punctuation in a string\n",
    "    * it splits the words on whitespace, and separated the punctuation out\n",
    "\n",
    "```python\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords \n",
    "import string\n",
    "\n",
    "# 1. Tokenization\n",
    "text = df.apply(lambda row: word_tokenize(row[\"email_body\"]), axis=1)\n",
    "text = text.rstrip()  # remove whitespace\n",
    "# replace with lowercase\n",
    "# text = re.sub(r'[^a-zA-Z]', ' ', text)\n",
    "text = text.lower()\n",
    "\n",
    " # 2. Remove all stopwords and punctuation\n",
    "exclude = set(string.punctuation)\n",
    "stop = set(stopwords.words('english'))\n",
    "stop_free = \" \".join([word for word in text if((word not in stop) and (not word.isdigit()))])\n",
    "punc_free = ''.join(word for word in stop_free if word not in exclude)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Preprocessing II\n",
    "\n",
    "```python\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "# Lemmatize words\n",
    "lemma = WordNetLemmatizer()\n",
    "normalized = \" \".join(lemma.lemmatize(word) for word in punc_free.split())\n",
    "\n",
    "# Stem words\n",
    "porter= PorterStemmer()\n",
    "cleaned_text = \" \".join(porter.stem(token) for token in normalized.split())\n",
    "print (cleaned_text)\n",
    "\n",
    "['philip','going','street','curious','hear','perspective','may','wish',\n",
    "'offer','trading','floor','enron','stock','lower','joined','company',\n",
    "'business','school','imagine','quite','happy','people','day','relate',\n",
    "'somewhat','stock','around','fact','broke','day','ago','knowing',\n",
    "'imagine','letting','event','get','much','taken','similar',\n",
    "'problem','hope','everything','else','going','well','family','knee',\n",
    "'surgery','yet','give','call','chance','later']\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing stopwords\n",
    "\n",
    "In the following sections, you're going to **clean the Enron emails**, in order to be able to use the data in a topic model. Text cleaning can be challenging, so you'll learn some steps to do this well. The dataframe containing the emails `df` is available. In a first step you need to **define the list of stopwords and punctuations** that are to be removed in the next section from the text data. Let's give it a try.\n",
    "\n",
    "**Instructions**\n",
    "\n",
    "* Import the stopwords from `ntlk`.\n",
    "* Define 'english' words to use as stopwords under the variable `stop`.\n",
    "* Get the punctuation set from the `string` package and assign it to `exclude`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/masoud/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/masoud/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/masoud/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"wordnet\")\n",
    "nltk.download(\"omw-1.4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define stopwords to exclude\n",
    "stop = set(stopwords.words(\"english\"))\n",
    "stop.update(\n",
    "    (\n",
    "        \"to\",\n",
    "        \"cc\",\n",
    "        \"subject\",\n",
    "        \"http\",\n",
    "        \"from\",\n",
    "        \"sent\",\n",
    "        \"ect\",\n",
    "        \"u\",\n",
    "        \"fwd\",\n",
    "        \"www\",\n",
    "        \"com\",\n",
    "        \"html\",\n",
    "    )\n",
    ")\n",
    "\n",
    "# Define punctuations to exclude and lemmatizer\n",
    "exclude = set(string.punctuation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning text data\n",
    "\n",
    "Now that you've defined the **stopwords and punctuations**, let's use these to clean our enron emails in the dataframe `df` further. The lists containing stopwords and punctuations are available under `stop` and `exclude` There are a few more steps to take before you have cleaned data, such as **\"lemmatization\"** of words, and **stemming the verbs**. The verbs in the email data are already stemmed, and the lemmatization is already done for you in this section.\n",
    "\n",
    "**Instructions 1/2**\n",
    "\n",
    "* Use the previously defined variables `stop` and `exclude` to finish of the function: Strip the words from whitespaces using `rstrip`, and exclude stopwords and punctuations. Finally lemmatize the words and assign that to `normalized`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the lemmatizer from nltk\n",
    "lemma = WordNetLemmatizer()\n",
    "\n",
    "\n",
    "def clean(text, stop):\n",
    "    text = str(text).rstrip()\n",
    "    stop_free = \" \".join(\n",
    "        [i for i in text.lower().split() if ((i not in stop) and (not i.isdigit()))]\n",
    "    )\n",
    "    punc_free = \"\".join(i for i in stop_free if i not in exclude)\n",
    "    normalized = \" \".join(lemma.lemmatize(i) for i in punc_free.split())\n",
    "    return normalized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Instructions 2/2**\n",
    "\n",
    "* Apply the function `clean(text,stop)` on each line of text data in our dataframe, and take the column `df['clean_content']` for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean the emails in df and print results\n",
    "text_clean = []\n",
    "for text in df[\"clean_content\"]:\n",
    "    text_clean.append(clean(text, stop).split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'investools advisory free digest trusted investment advice unsubscribe free newsletter please see issue fried sells stocks gains months km rowe january index confirms bull market aloy small cap advisor earns lbix compounding returns pine trees pcl undervalued high yield bank puts customers first aso word sponsor top wall street watcher ben zacks year year gain moving best brightest wall street big money machines earned ben zacks five year average annual gain start outperforming long term get zacks latest stock buylist free day trial http www investools com c go zaks mtxtu zakstb investools advisory john brobst investools com fried sells stocks locks months km david fried knows stock undervalued company management buys back shares open market latest triumph pocketing impressive gain three short months selling four buyback stocks include gain auto retailer automation incorporated gain digital phone system purveyor inter tel intl fried recent move buy kmart corporation km beleaguered discount retailer declared bankruptcy think k mart go business fried says take recovery possibility bought shares another fried pick c cor net corporation ccbl provides range technology services broadband networks today telecom spending slowdown hit company hard net sales fell million last quarter caused net loss million vs million gain last year fried cites buyback plan million restructuring charge proof management sees rosier future david fried advice see buyback index portfolios january buyback letter david fried provides wealth building opportunities companies repurchasing stock free day trial go http www investools com c go back mtxtu back rowe january index confirms bull market aloy rowe says january index confirms see bull market first five trading days provided gains nasdaq p dow industrials rowe says five day index correctly predicted market direction year since four exceptions include three war years fed funds rate doubled year rowe maintains sure recommendations seven companies says leading market one alloy incorporated aloy media company direct marketer provides content community commerce generation roughly million people years age rowe likes market accounts billion disposable income grow faster overall population q saw earnings increase sales another rowe pick new century financial corporation ncen financier makes buys sells services sub prime mortgage loans secured first mortgages single family homes borrowers typically plenty equity properties secure loan suffer weak credit profiles high debt income ratios q earnings grew hike sales rowe advice see investment opportunity february wall street digest momentum investor donald rowe targets stocks mutual funds capable generating annual returns free day trial go http www investools com c go wall mtxtu wall small cap advisor earns lbix major indexes suffered terrible year richard geist recommendations earned healthy lists many reasons selections see growth going forward include extremely bullish monetary conditions high productivity inflation sight yield curve continues steepen also investor sentiment polls becoming bearish always contrary indicator says geist latest recommendation buy shares leading brands lbix company canada largest independent food brand management company expanding us geist particularly likes firm saves money integrated distribution system system makes products raw materials provides packaging warehousing distribution recent financial results show leading brands roll fy saw revenues grow million net income million per share last year loss geist predicts company see revenues reach million million yields forward pe think lbix significantly undervalued geist says range leading brands strong buy richard geist advice see highlighted stocks february richard geist strategic investing richard geist integrates psychological aspects investing methodology selecting small company stocks free day trial go http www investools com c go stin mtxtu stin compounding returns pine trees pcl growing trees usually noisy business catches attention investment media good business says dick young timber business less volatile capital intensive manufacturing young sees demand timber increasing population increases notes average return timber investments outperformed p average annual return young favorite timber play plum creek timber pcl one largest private timberland owners us reit primary goal profit acquiring managing lands young says plum creek timber yield status reit makes ideal tax deferred accounts another young timber selections deltic timber corporation del company grows harvests timber acres arkansas louisiana main company goal expand timber holdings sustainable harvest level young says shares good portfolio counterweight value investors appreciate intrinsic worth underlying real natural resources dick young advice see investment commentary february richard young intelligence report richard young uses buy hold strategy mentor warren buffett uncover low risk high reward opportunities free day trial go http www investools com c go inte mtxtu inte undervalued high yield bank puts customers first aso amsouth bancorp aso giving investors healthy yield risk involved says jodie weiss investment quality trends billion assets amsouth one largest financial institutions south offices credits bank success putting customer first weiss likes amsouth uses new technology save money streamlining operations notes amsouth ranked number six eweek fast track list companies deploy cutting edge technology throughout operations number merrill lynch financial services firm placed higher also amsouth internet banking group quadrupled customer base last year weiss says aso shares undervalued stock selling near yield weiss sees upside potential dividends risen annually past years buyback plan million shares authorized september stock pe reasonable x priced yield aso undervalued buy considered weiss says jodie weiss advice see investment spotlight january income digest digest excerpts investment publications highlights weather income oriented opportunities uncovered top minds wall street free day trial go http www investools com c go indi mtxtu indi word sponsor new report top picks despite slumping economy shaky stock market frank curzio bulls eye picks gained whopping curzio selected stocks incredible potential get red hot picks today click http www investools com c go fxcp fxcp mtxtu disclaimer investools advisory published solely informational purposes solicit offer buy sell stock mutual fund security attempt claim complete description securities markets developments referred material expressions opinion subject change without notice information obtained internal external sources investools considers reliable investools independently verified information investools guarantee accurate complete investools undertake advise anyone investools employees officers directors may time time position securities mentioned may sell buy securities remove free email list removed email distribution list free investools advisory updates simply click link hit send email launched copy paste email address new outgoing email message hit send email launched mailto u bonnie investools com important automated system cancel paid newsletter service subscriptions investools com tried unsubscribing past believe received message error please send email mailto itfeedback investools com voice concerns removed list paid subscription information questions investools services paid subscriptions contact investools customer service center http www investools com cgi bin help pl info pr faq html'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"clean_content\"].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['investools',\n",
       " 'advisory',\n",
       " 'free',\n",
       " 'digest',\n",
       " 'trusted',\n",
       " 'investment',\n",
       " 'advice',\n",
       " 'unsubscribe',\n",
       " 'free',\n",
       " 'newsletter']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_clean[0][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now that you have cleaned your data entirely with the necessary steps, including splitting the text into words, removing stopwords and punctuations, and lemmatizing your words. You are now ready to run a topic model on this data. In the following sections you're going to explore how to do that.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic modeling on fraud\n",
    "\n",
    "1. Discovering topics in text data\n",
    "1. \"What is the text about\"\n",
    "1. Conceptually similar to clustering data\n",
    "1. Compare topics of fraud cases to non-fraud cases and use as a feature or flag\n",
    "1. Or.. is there a particular topic in the data that seems to point to fraud?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Latent Dirichlet Allocation (LDA)\n",
    "\n",
    "* With LDA you obtain:\n",
    "    * \"topics per text item\" model (i.e. probabilities)\n",
    "    * \"words per topic\" model\n",
    "* Creating your own topic model:\n",
    "    * Clean your data\n",
    "    * Create a bag of words with dictionary and corpus\n",
    "        * Dictionary contain words and word frequency from the entire text\n",
    "        * Corpus: word count for each line of text\n",
    "    * Feed dictionary and corpus into the LDA model\n",
    "* LDA:\n",
    "    * ![lda](https://raw.githubusercontent.com/trenton3983/DataCamp/master/Images/fraud_detection/lda.JPG)\n",
    "    1. [LDA2vec: Word Embeddings in Topic Models](https://www.datacamp.com/community/tutorials/lda2vec-topic-model)\n",
    "    1. see how each word in the dataset is associated with each topic\n",
    "    1. see how each text item in the data associates with topics (in the form of probabilities)\n",
    "        1. image on the right "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bag of words: dictionary and corpus\n",
    "\n",
    "* use the `Dictionary` function in `corpora` to create a `dict` from the text data\n",
    "    * contains word counts\n",
    "* filter out words that appear in less than 5 emails and keep only the 50000 most frequent words\n",
    "    * this is a way of cleaning the outlier noise\n",
    "* create the corpus, which for each email, counts the number of words and the count for each word (`doc2bow`)\n",
    "* `doc2bow`\n",
    "    * Document to Bag of Words\n",
    "    * converts text data into bag-of-words format\n",
    "    * each row is now a list of words with the associated word count\n",
    "    \n",
    "```python\n",
    "from gensim import corpora\n",
    "\n",
    " # Create dictionary number of times a word appears\n",
    "dictionary = corpora.Dictionary(cleaned_emails)\n",
    "\n",
    "# Filter out (non)frequent words \n",
    "dictionary.filter_extremes(no_below=5, keep_n=50000)\n",
    "\n",
    "# Create corpus\n",
    "corpus = [dictionary.doc2bow(text) for text in cleaned_emails]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Latent Dirichlet Allocation (LDA) with gensim\n",
    "\n",
    "* Run the LDA model after cleaning the text date, and creating the dictionary and corpus\n",
    "* Pass the corpus and dictionary into the model\n",
    "* As with K-means, beforehand, pick the number of topics to obtain, even if there is uncertainty about what topics exist\n",
    "* The calculated LDA model, will contain the associated words for each topic, and topic scores per email\n",
    "* Use `print_topics` to obtain the top words from the topics\n",
    "\n",
    "```python\n",
    "import gensim\n",
    "\n",
    "# Define the LDA model\n",
    "ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics = 3, \n",
    "id2word=dictionary, passes=15)\n",
    "\n",
    "# Print the three topics from the model with top words\n",
    "topics = ldamodel.print_topics(num_words=4)\n",
    "for topic in topics:\n",
    "    print(topic)\n",
    "\n",
    ">>> (0, '0.029*\"email\" + 0.016*\"send\" + 0.016*\"results\" + 0.016*\"invoice\"')\n",
    ">>> (1, '0.026*\"price\" + 0.026*\"work\" + 0.026*\"management\" + 0.026*\"sell\"')\n",
    ">>> (2, '0.029*\"distribute\" + 0.029*\"contact\" + 0.016*\"supply\" + 0.016*\"fast\"')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create dictionary and corpus\n",
    "\n",
    "In order to run an LDA topic model, you first need to **define your dictionary and corpus** first, as those need to go into the model. You're going to continue working on the cleaned text data that you've done in the previous sections. That means that `text_clean` is available for you already to continue working with, and you'll use that to create your dictionary and corpus.\n",
    "\n",
    "This section will take a little longer to execute than usual.\n",
    "\n",
    "**Instructions**\n",
    "\n",
    "* Import the gensim package and corpora from gensim separately.\n",
    "* Define your dictionary by running the correct function on your clean data `text_clean`.\n",
    "* Define the corpus by running `doc2bow` on each piece of text in `text_clean`.\n",
    "* Print your results so you can see `dictionary` and `corpus` look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the dictionary\n",
    "dictionary = corpora.Dictionary(text_clean)\n",
    "\n",
    "# Define the corpus\n",
    "corpus = [dictionary.doc2bow(text) for text in text_clean]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary<33980 unique tokens: ['account', 'accurate', 'acquiring', 'acre', 'address']...>\n"
     ]
    }
   ],
   "source": [
    "print(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 2),\n",
       " (1, 1),\n",
       " (2, 1),\n",
       " (3, 1),\n",
       " (4, 1),\n",
       " (5, 6),\n",
       " (6, 1),\n",
       " (7, 2),\n",
       " (8, 4),\n",
       " (9, 1)]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus[0][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**These are the two ingredients you need to run your topic model on the enron emails. You are now ready for the final step and create your first fraud detection topic model.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA model\n",
    "\n",
    "Now it's time to **build the LDA model**. Using the `dictionary` and `corpus`, you are ready to discover which topics are present in the Enron emails. With a quick print of words assigned to the topics, you can do a first exploration about whether there are any obvious topics that jump out. Be mindful that the topic model is **heavy to calculate** so it will take a while to run. Let's give it a try!\n",
    "\n",
    "**Instructions**\n",
    "\n",
    "* Build the LDA model from gensim models, by inserting the `corpus` and `dictionary`.\n",
    "* Save the 5 topics by running `print_topics` on the model results, and select the top 5 words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, '0.008*\"email\" + 0.008*\"enron\" + 0.006*\"request\" + 0.005*\"time\" + 0.005*\"day\"')\n",
      "(1, '0.037*\"image\" + 0.036*\"td\" + 0.026*\"net\" + 0.025*\"money\" + 0.024*\"tr\"')\n",
      "(2, '0.032*\"enron\" + 0.009*\"company\" + 0.006*\"market\" + 0.005*\"energy\" + 0.005*\"development\"')\n",
      "(3, '0.029*\"enron\" + 0.012*\"hou\" + 0.011*\"pm\" + 0.010*\"e\" + 0.007*\"outage\"')\n",
      "(4, '0.013*\"enron\" + 0.011*\"message\" + 0.008*\"pm\" + 0.008*\"original\" + 0.007*\"please\"')\n"
     ]
    }
   ],
   "source": [
    "# Define the LDA model\n",
    "ldamodel = gensim.models.ldamodel.LdaModel(\n",
    "    corpus, num_topics=5, id2word=dictionary, passes=5\n",
    ")\n",
    "\n",
    "# Save the topics and top 5 words\n",
    "topics = ldamodel.print_topics(num_words=5)\n",
    "\n",
    "# Print the results\n",
    "for topic in topics:\n",
    "    print(topic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**You have now successfully created your first topic model on the Enron email data. However, the print of words doesn't really give you enough information to find a topic that might lead you to signs of fraud. You'll therefore need to closely inspect the model results in order to be able to detect anything that can be related to fraud in your data.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flagging fraud based on topic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using your LDA model results for fraud detection\n",
    "\n",
    "1. Are there any suspicious topics? (no labels)\n",
    "    1. if you don't have labels, first check for the frequency of suspicious words within topics and check whether topics seem to describe the fraudulent behavior\n",
    "    1. for the Enron email data, a suspicious topic would be one where employees are discussing stock bonuses, selling stock, stock price, and perhaps mentions of accounting or weak financials\n",
    "    1. Defining suspicious topics does require some pre-knowledge about the fraudulent behavior\n",
    "    1. If the fraudulent topic is noticeable, *flag all instances that have a high probability for this topic*\n",
    "1. Are the topics in fraud and non-fraud cases similar? (with labels)\n",
    "    1. If there a previous cases of fraud, ran a topic model on the fraud text only, and on the non-fraud text\n",
    "    1. Check whether the results are similar\n",
    "        1. Whether the frequency of the topics are the same in fraud vs non-fraud\n",
    "1. Are fraud cases associated more with certain topics? (with labels)\n",
    "    1. Check whether fraud cases have a higher probability score for certain topics\n",
    "        1. If so, run a topic model on new data and create a flag directly on the instances that score high on those topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### To understand topics, you need to visualize\n",
    "\n",
    "```python\n",
    "import pyLDAvis\n",
    "from pyLDAvis import gensim_models as gensimvis\n",
    "pyLDAvis.enable_notebook()\n",
    "lda_display = gensimvis.prepare(ldamodel, corpus, dictionary, sort_topics=False)\n",
    "```\n",
    "\n",
    "![topics](https://raw.githubusercontent.com/trenton3983/DataCamp/master/Images/fraud_detection/topics2.jpg)\n",
    "\n",
    "* Each bubble on the left-hand side, represents a topic\n",
    "* The larger the bubble, the more prevalent that topic is\n",
    "* Click on each topic to get the details per topic in the right panel\n",
    "* The words are the most important keywords that form the selected topic.\n",
    "* A good topic model will have fairly big, non-overlapping bubbles, scattered throughout the chart\n",
    "* A model with too many topics, will typically have many overlaps, or small sized bubbles, clustered in one region\n",
    "* In the case of the model above, there is a slight overlap between topic 2 and 3, which may point to 1 topic too many"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyLDAvis\n",
    "from pyLDAvis import gensim_models as gensimvis\n",
    "\n",
    "pyLDAvis.enable_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_display = gensimvis.prepare(ldamodel, corpus, dictionary, sort_topics=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<link rel=\"stylesheet\" type=\"text/css\" href=\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.3.1/pyLDAvis/js/ldavis.v1.0.0.css\">\n",
       "\n",
       "\n",
       "<div id=\"ldavis_el378231401055981753129857172019\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "\n",
       "var ldavis_el378231401055981753129857172019_data = {\"mdsDat\": {\"x\": [-0.030936459542180068, 0.3281914565890246, -0.0654385417809095, -0.13547862592854176, -0.09633782933739322], \"y\": [0.1452684661209786, -0.04408605507027358, 0.06349537821496892, -0.1853080030977895, 0.020630213832115284], \"topics\": [1, 2, 3, 4, 5], \"cluster\": [1, 1, 1, 1, 1], \"Freq\": [5.9313678106404355, 3.7787093733498778, 63.129600364735985, 6.5852539640901115, 20.575068487183596]}, \"tinfo\": {\"Term\": [\"enron\", \"image\", \"money\", \"td\", \"net\", \"pm\", \"hou\", \"e\", \"message\", \"tr\", \"original\", \"width\", \"time\", \"corp\", \"please\", \"class\", \"day\", \"right\", \"bakernet\", \"email\", \"mail\", \"year\", \"thanks\", \"team\", \"desk\", \"table\", \"height\", \"know\", \"request\", \"energy\", \"classmate\", \"vladimir\", \"gorny\", \"blah\", \"validation\", \"emailed\", \"lover\", \"vegetarian\", \"veggie\", \"srrs\", \"emaillink\", \"itcapps\", \"redshirt\", \"auth\", \"adult\", \"pep\", \"filet\", \"mawhitt\", \"strick\", \"jurgens\", \"todaysheadlines\", \"lubbock\", \"diabetes\", \"frenzy\", \"freshman\", \"franklin\", \"perfmgmt\", \"fish\", \"dickerman\", \"galore\", \"headcount\", \"strict\", \"reviewer\", \"curve\", \"simulation\", \"stack\", \"nguyen\", \"feedback\", \"membership\", \"nytimes\", \"football\", \"holiday\", \"game\", \"friend\", \"lavorato\", \"email\", \"request\", \"sept\", \"find\", \"id\", \"click\", \"desk\", \"make\", \"get\", \"day\", \"wish\", \"play\", \"time\", \"available\", \"friday\", \"john\", \"product\", \"received\", \"enron\", \"help\", \"like\", \"message\", \"send\", \"group\", \"year\", \"new\", \"know\", \"image\", \"width\", \"td\", \"img\", \"src\", \"href\", \"scoop\", \"nbsp\", \"bodydefault\", \"height\", \"cellpadding\", \"cellspacing\", \"script\", \"align\", \"ctr\", \"syncrasy\", \"valign\", \"bgcolor\", \"tr\", \"colspan\", \"coords\", \"rect\", \"stocklookup\", \"gif\", \"ecar\", \"npcc\", \"std\", \"wscc\", \"linkbarseperator\", \"br\", \"frcc\", \"hp\", \"rk\", \"table\", \"font\", \"sp\", \"image\", \"sw\", \"class\", \"amazon\", \"net\", \"money\", \"ne\", \"border\", \"se\", \"clear\", \"click\", \"center\", \"right\", \"euci\", \"electricity\", \"asset\", \"declared\", \"donate\", \"brochure\", \"profit\", \"partnership\", \"retirement\", \"design\", \"investor\", \"congestion\", \"mr\", \"aggressively\", \"astronomical\", \"wiped\", \"pocketbook\", \"underhanded\", \"competitive\", \"netted\", \"devastated\", \"financially\", \"repair\", \"chairman\", \"crisis\", \"ferc\", \"afford\", \"bankrupt\", \"ceo\", \"buying\", \"million\", \"lay\", \"largest\", \"ken\", \"transition\", \"said\", \"earnings\", \"sold\", \"accounting\", \"regulatory\", \"company\", \"development\", \"stock\", \"york\", \"reported\", \"market\", \"business\", \"industry\", \"investment\", \"share\", \"board\", \"consumer\", \"employee\", \"fund\", \"enron\", \"energy\", \"conference\", \"service\", \"year\", \"power\", \"new\", \"plan\", \"many\", \"trading\", \"state\", \"management\", \"california\", \"time\", \"issue\", \"made\", \"also\", \"would\", \"price\", \"one\", \"e\", \"please\", \"gas\", \"bill\", \"corp\", \"aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa\", \"outage\", \"pager\", \"fri\", \"thru\", \"bergfelt\", \"backout\", \"pt\", \"caiso\", \"eei\", \"epe\", \"agave\", \"epenergy\", \"eix\", \"gregwhalley\", \"darrell\", \"reliantenergy\", \"schoolcraft\", \"sfs\", \"powersrc\", \"pampa\", \"pcg\", \"chanley\", \"graf\", \"hanagriff\", \"sat\", \"demoes\", \"neumin\", \"royalty\", \"unify\", \"devon\", \"breese\", \"kowalke\", \"impacted\", \"sdg\", \"server\", \"lynn\", \"ref\", \"scheduled\", \"bradford\", \"blair\", \"hou\", \"whalley\", \"ben\", \"pm\", \"ct\", \"e\", \"enron\", \"december\", \"mail\", \"london\", \"greg\", \"corp\", \"original\", \"message\", \"team\", \"doc\", \"thanks\", \"mark\", \"january\", \"please\", \"williams\", \"time\", \"energy\", \"credit\", \"point\", \"bakernet\", \"hourahead\", \"beth\", \"tyrell\", \"laurel\", \"variance\", \"kern\", \"wj\", \"savita\", \"enform\", \"tagline\", \"detected\", \"exotica\", \"txt\", \"bolt\", \"westdesk\", \"parsing\", \"addin\", \"krishna\", \"fran\", \"locate\", \"interchange\", \"baughman\", \"assign\", \"nella\", \"maggi\", \"evelyn\", \"permian\", \"cherry\", \"fagan\", \"pseg\", \"harrison\", \"schedule\", \"recipient\", \"tycholiz\", \"final\", \"eol\", \"matt\", \"stsw\", \"intended\", \"mailto\", \"file\", \"message\", \"date\", \"original\", \"jason\", \"permit\", \"error\", \"confidential\", \"revised\", \"pm\", \"alert\", \"know\", \"thanks\", \"team\", \"barry\", \"let\", \"november\", \"attached\", \"tuesday\", \"please\", \"copy\", \"thursday\", \"agreement\", \"see\", \"monday\", \"number\", \"call\", \"enron\", \"mail\", \"may\", \"contact\", \"gas\", \"would\", \"e\", \"use\", \"corp\", \"also\"], \"Freq\": [11196.0, 730.0, 683.0, 608.0, 699.0, 1327.0, 1034.0, 1660.0, 1419.0, 402.0, 1050.0, 355.0, 1602.0, 1222.0, 1756.0, 325.0, 950.0, 505.0, 537.0, 525.0, 721.0, 1105.0, 933.0, 609.0, 477.0, 232.0, 225.0, 976.0, 398.0, 1611.0, 109.63097105664191, 39.280800479286135, 38.965208542412256, 35.76302802306119, 36.46005210168353, 36.44533126016989, 36.613815296653875, 35.65137136918014, 35.65137136918014, 27.061359809934768, 25.535171311828023, 25.53478600529968, 23.89613978124111, 30.645152179217483, 74.00423189216295, 70.13569342958357, 23.820881649207372, 23.820881649207372, 23.820881649207372, 24.05308898908612, 31.078206258059513, 24.556416582304728, 24.71122909031075, 13.240035423590605, 12.955537571258835, 15.062693689520401, 13.586989484330655, 12.956228990878863, 12.38371298566093, 12.383597241407356, 35.94062539656334, 24.50905433833279, 14.775125568567537, 73.76607895273223, 94.39200952958474, 47.85705674776478, 24.819935788676386, 99.98324891152798, 53.672084598090045, 43.79290135894277, 45.39432971367479, 77.61433780372506, 76.94279573629724, 67.81754924361474, 44.05068817886116, 219.02731023588285, 169.09540981496252, 59.18307666149027, 95.94295816025188, 90.11165301279256, 96.04513596894557, 115.10307914264442, 115.39123359961867, 109.78573939907749, 125.46162939156997, 48.93603381727667, 65.0141685958509, 139.34221635869423, 84.86541679173638, 81.0111514231634, 89.84626362295528, 82.7823065421546, 66.26319529517141, 204.12658996858295, 84.6619323021522, 95.78533885409671, 109.47463102907886, 72.47215566663102, 83.46822526423676, 87.83974561022474, 85.30850406982235, 79.9376965729075, 77.68363206079839, 354.1301708419394, 605.2215327145929, 176.77794405606625, 187.8900680514417, 159.57069854544497, 111.86147073950652, 132.77427890179638, 93.47277286969053, 222.98238469614452, 77.77080352060413, 77.85969982952555, 181.1751639231359, 181.31680190839722, 62.29358589780157, 62.29358589780157, 49.0101115372954, 43.66656861218854, 392.85859722883475, 40.1100138091639, 38.10779773369984, 38.10779773369984, 38.107809376454, 171.60618608765625, 34.62723518707049, 34.62723518707049, 34.62723518707049, 101.47352605281606, 31.187183603537115, 200.10477638933932, 31.168941348229108, 71.182176703906, 69.47276425278467, 215.11155688759646, 78.10908762360968, 109.06265354357093, 614.9669836557773, 70.88219726208331, 255.38834538825083, 66.75702198869105, 441.3479361344377, 412.4259066258307, 109.25317557262754, 130.6335179327592, 117.3308330278908, 131.31708731493785, 119.31692388884476, 96.70350519757773, 86.18851477750509, 238.13557928301938, 302.72962181052674, 527.273791282331, 270.1758048474139, 267.33780105872506, 175.72283731151418, 279.8361697214544, 231.7380313072656, 321.8743186537606, 214.80151002521222, 362.9588101345221, 154.97466873290344, 653.7035412159979, 137.94154007397262, 131.9197072640984, 134.47264634918736, 131.8863971702254, 131.0403369951424, 128.11736410081747, 136.12950338586387, 133.55982010273559, 136.15845318034425, 135.26246838768034, 233.80424589416882, 202.04649965139228, 258.4231284272801, 136.0736947954966, 144.37249539934794, 154.76412624661805, 208.67191784596196, 994.649517645843, 491.40104085402703, 242.0665755923198, 273.45979108029826, 277.558731858026, 1111.338715340803, 204.08640655831695, 203.9545277633044, 214.38181905170376, 176.356944791958, 2559.6086898119543, 1379.6295903165817, 1253.632304278064, 454.8319392662173, 246.09390372054344, 1672.6397643858334, 940.2530159101142, 354.5001210800803, 373.4678448795645, 491.33717624582283, 366.07092925045487, 511.59879052380774, 1224.902048440704, 669.4471721144035, 8978.300065297079, 1445.2921331253615, 814.7354600605335, 1098.9450911192453, 988.2893807146922, 1076.9225642582978, 1377.3684590003243, 638.7498863443539, 447.5740021017632, 748.8408650357509, 592.5045185676183, 626.7521578564964, 644.8734972950692, 1251.56425804123, 552.0547505203747, 620.061092031666, 764.426682272868, 899.3009543402736, 622.945956881857, 656.5920369474675, 861.7415623023046, 865.9907956301007, 721.886628840959, 626.8784606552597, 674.6272077928388, 91.78768508387337, 206.96128355336336, 62.27117746723038, 125.9752166477659, 143.6635541654549, 48.23657441907589, 41.83349392787047, 86.08717391091038, 43.41292813673814, 48.82901230979867, 40.44283229698058, 32.19692807698783, 32.50806702904977, 32.19209902676834, 32.19209902676834, 72.66447036693326, 50.42395832073692, 64.64207880398327, 63.665427068906595, 32.81573975108721, 24.171198788150846, 32.4454516972122, 24.173314033887745, 24.454115864437757, 47.776120422622306, 119.30237553677142, 24.14106118622082, 40.015539131375945, 24.080133316609814, 19.31393683795117, 103.56623229562793, 40.50213019305812, 32.74828522396667, 42.45247806804912, 34.36533056486596, 64.39422137626117, 93.33329226938982, 53.98465579472624, 152.79785821325686, 45.379984931291105, 57.30794863807218, 348.49618697136737, 93.68724677105185, 66.81102574360183, 333.54755766503723, 83.55609554845981, 299.4219177517869, 837.0094000154182, 104.37727601435391, 173.09757565257846, 88.60484049943203, 134.18525130495885, 201.59148792330305, 185.87345700217048, 197.46136494426577, 134.73039290970843, 93.29181924985774, 137.18000641236384, 112.58371255569705, 97.6504158219826, 141.79866154189702, 91.77709363759365, 117.37873623225207, 106.90644710486905, 90.92796535054, 89.23539001198507, 535.6602703217703, 149.90805008726105, 159.88454006087503, 108.16915680548466, 113.62287271028534, 100.78383353220065, 93.34918911703113, 433.65299142060076, 110.47623034749905, 90.88636515673609, 90.15569800893202, 73.91320419842333, 124.95295754424475, 74.36659312718264, 73.38278004141058, 70.66065028331053, 70.66209779754325, 73.2841377576396, 73.71609924943579, 75.03849331029795, 57.67697590572487, 56.29107083717768, 57.17299265650313, 58.09270833283895, 54.18436707552244, 49.86890979880556, 49.344983607144044, 55.93065564192768, 90.42967969921324, 53.88825532275639, 109.5932255339484, 111.68497869239712, 435.8508040525385, 287.4318657420375, 125.26357085383125, 408.8942853523543, 164.4481509363266, 87.22350435143257, 69.89190400196847, 284.904357970555, 440.23167946335644, 359.63252324779404, 999.8727595915528, 406.93437221285313, 745.5593432830012, 182.48068099704022, 101.84673161090606, 146.0529393168381, 126.7931073711341, 136.17470069249427, 747.5894133167011, 155.59272375565962, 541.2592131107218, 513.7679805398134, 366.15639593405837, 155.97558598731402, 365.263501534217, 249.79178334584876, 221.1303253270676, 222.40665251161855, 628.0680152002789, 223.06458471067805, 272.466702866825, 228.4848598669638, 305.45779217942663, 234.12452323878307, 216.49715980786445, 290.96369481188833, 1176.739473235854, 310.6329831010989, 362.95111339615113, 246.17447697568645, 332.819319022446, 350.1352872747765, 369.2460463861055, 249.2237238187854, 308.2185927557145, 246.9575082150039], \"Total\": [11196.0, 730.0, 683.0, 608.0, 699.0, 1327.0, 1034.0, 1660.0, 1419.0, 402.0, 1050.0, 355.0, 1602.0, 1222.0, 1756.0, 325.0, 950.0, 505.0, 537.0, 525.0, 721.0, 1105.0, 933.0, 609.0, 477.0, 232.0, 225.0, 976.0, 398.0, 1611.0, 110.9140355135829, 40.169112155221086, 39.87047913439592, 36.67217510041219, 37.397021234092, 37.429933164917934, 37.648821528167176, 36.705756051186874, 36.705756051186874, 27.930771930252114, 26.397656653543926, 26.398281327036006, 24.760373594163664, 31.760035557378657, 76.78055410836896, 72.86745439136442, 24.80614325803235, 24.80614325803235, 24.80614325803235, 25.19396170484221, 32.70560748253519, 25.954398603535637, 26.21507312112824, 14.129499671627924, 13.828403243652808, 16.078510212275386, 14.515529168834668, 13.846921782885966, 13.241335563051871, 13.241251220418855, 38.58364991021518, 26.297993576353992, 15.80701752915078, 80.84563913405213, 105.33260719522481, 52.5061781879279, 27.121810914662596, 118.85868771237396, 62.84734355072698, 50.93033065474104, 53.84126382404319, 106.49133758497042, 107.91129188007467, 94.89858161194871, 56.70046841862321, 525.2244404617848, 398.1546343176991, 93.37007590295057, 236.15161619928216, 243.84784207411906, 287.77268718635895, 477.2099086458708, 595.9954142455531, 661.408588079441, 950.1548264599014, 83.58849505033868, 186.50985258687192, 1602.452500608868, 419.99272487843075, 376.85176593087215, 522.208227508736, 418.76395181905315, 207.84187797739048, 11196.622088232518, 505.1771196007628, 798.6891945652134, 1419.9274181538112, 310.2379471128148, 779.5377107226304, 1105.7478287316235, 1610.9068098440387, 976.969952641663, 730.2700405393127, 355.4571901152707, 608.6171283135576, 177.87789811457552, 189.08568465520545, 160.92108551540585, 112.88086485216589, 134.15381049648585, 94.44570230889266, 225.85268489760898, 78.82084881429421, 78.91634125724738, 183.80951296891106, 184.23220721370055, 63.390550753342474, 63.390550753342474, 50.12069684412593, 44.741534371769525, 402.5664329429553, 41.1151447928012, 39.079827363719176, 39.079827363719176, 39.07988802396906, 176.00014159949825, 35.66843467535284, 35.66843467535284, 35.66843467535284, 104.61714055003979, 32.159155880819334, 206.6023563892401, 32.20317016669264, 73.58176473716443, 72.49611739809205, 232.95619031999388, 82.42004810992641, 116.88425285898856, 730.2700405393127, 75.97319951305549, 325.3024049691709, 73.27957432990131, 699.3730908543838, 683.72908025221, 136.2543599364088, 179.34161374596943, 167.0049992708673, 225.3293679962149, 287.77268718635895, 293.6336053218122, 505.3064102538333, 238.71856045220608, 303.6235924631065, 528.8776824505816, 271.0470950247439, 268.3130345316021, 176.36564445107038, 281.0302948965555, 232.73857728383524, 323.3488131929499, 215.81874590778625, 364.679918133154, 155.71200069757415, 656.9551400468888, 138.6323620223796, 132.598128701758, 135.16898482800016, 132.57500478922296, 131.72511965999007, 128.80273259745985, 136.85938331768324, 134.278607695976, 136.89200045982903, 135.99652323756249, 235.0793631706168, 203.1704187266993, 259.8735565392643, 136.84390021037856, 145.20860936119803, 155.67830633635774, 209.94028108883563, 1002.8069636469718, 495.28773417775585, 243.55710379700645, 275.6433467746539, 279.92832167471505, 1137.566401820962, 205.49474420371573, 205.41926213726452, 216.2846621040053, 177.69483756746567, 2662.0827567775696, 1426.267978025897, 1299.5771449351614, 463.05121497091125, 249.15732356338106, 1765.2975696616654, 982.9237467657161, 361.68734821287615, 382.4355324023038, 507.890520156871, 375.17791938744494, 531.9098947725387, 1330.8143077832797, 707.1280728247742, 11196.622088232518, 1611.129674677574, 878.1386074882613, 1237.5419074647102, 1105.7478287316235, 1224.0913750553261, 1610.9068098440387, 693.7731962555669, 472.84978750324393, 852.1396924027993, 654.5796099899292, 703.9941758532533, 728.2312897356385, 1602.452500608868, 610.7242973632054, 756.0183815505313, 1053.5359348685186, 1373.1891728765459, 792.0305572943562, 884.9680939177837, 1660.4708507029557, 1756.7867709381103, 1153.0366317564824, 812.6127046962473, 1222.6751677057646, 93.18992068537565, 210.2027281347634, 63.250121725613724, 128.14762308532127, 146.355063770626, 49.24168748076618, 42.75371880957629, 88.03410280221746, 44.537188034169795, 50.1767098965724, 41.58146626933913, 33.135561861388545, 33.46393944682426, 33.165325597181265, 33.165325597181265, 74.8978407925127, 51.990547905740705, 66.70843214980043, 66.05163891445376, 34.05753016185509, 25.09240124899441, 33.68288282598487, 25.1010191716116, 25.465422417152578, 49.78912359653137, 124.33782531256969, 25.217820079713373, 41.81886037990844, 25.216169128579146, 20.253143170336063, 108.87964757239597, 42.57741250657112, 34.48197900209005, 45.06723422327621, 36.312877447953376, 71.07107941191033, 108.76780188314135, 59.81297480805153, 207.40230681412982, 49.97560759515255, 70.75592049168934, 1034.5788181536411, 155.86136694392155, 96.67526128756923, 1327.9545296567217, 145.56234431034304, 1660.4708507029557, 11196.622088232518, 245.15006589839024, 721.3005642664054, 189.85208768032038, 486.43541369641883, 1222.6751677057646, 1050.496834440603, 1419.9274181538112, 609.7378143420019, 256.0427672371713, 933.1027983890748, 545.6228483495028, 360.60707706333005, 1756.7867709381103, 290.28447339487326, 1602.452500608868, 1611.129674677574, 354.1070009696966, 309.18111254731156, 537.6879824851998, 150.70179764200304, 160.83567837374147, 108.95655203749735, 114.46518522515204, 101.5802602501494, 94.13603147440385, 437.4360792691395, 111.4525605656002, 91.7122356601516, 90.98255830947751, 74.69826144568216, 126.28567986713112, 75.16756505731855, 74.17527545738534, 71.44898203003861, 71.4506906752872, 74.12567284162009, 74.57084255446888, 75.93044870446504, 58.45702354347639, 57.0666541532933, 57.973782767569126, 58.91851911872258, 54.990865939317565, 50.67067641182253, 50.1414712925089, 56.86722695502866, 91.96643718861075, 54.80606856323678, 111.8323251748117, 115.08596125428579, 469.4065721006656, 308.5546983861733, 131.25979748543847, 457.90269539448275, 175.74190314484218, 90.3215247474847, 71.69698285914771, 326.41366382430914, 524.1369177016686, 436.1083961807766, 1419.9274181538112, 519.436813386161, 1050.496834440603, 215.03134999274897, 109.19493603856868, 169.02954479682893, 142.90450275534224, 156.48408549017603, 1327.9545296567217, 186.998184608113, 976.969952641663, 933.1027983890748, 609.7378143420019, 189.951536131181, 643.2219805679083, 383.793006569455, 324.9254155898992, 333.3556880494888, 1756.7867709381103, 344.7889257593215, 485.80194734673773, 363.75743672754265, 603.9518768980131, 396.0991133825684, 349.59016873933825, 618.759433818147, 11196.622088232518, 721.3005642664054, 1012.5233133317513, 464.3697459601542, 1153.0366317564824, 1373.1891728765459, 1660.4708507029557, 500.85763761676935, 1222.6751677057646, 1053.5359348685186], \"Category\": [\"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\"], \"logprob\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, -5.475, -6.5014, -6.5095, -6.5952, -6.5759, -6.5763, -6.5717, -6.5984, -6.5984, -6.874, -6.9321, -6.9321, -6.9984, -6.7497, -5.868, -5.9217, -7.0016, -7.0016, -7.0016, -6.9919, -6.7356, -6.9712, -6.9649, -7.5889, -7.6106, -7.4599, -7.563, -7.6106, -7.6558, -7.6558, -6.5903, -6.9731, -7.4792, -5.8712, -5.6247, -6.3039, -6.9605, -5.5671, -6.1893, -6.3927, -6.3568, -5.8204, -5.8291, -5.9553, -6.3868, -4.7829, -5.0417, -6.0915, -5.6084, -5.6711, -5.6073, -5.4263, -5.4238, -5.4736, -5.3401, -6.2816, -5.9975, -5.2352, -5.7311, -5.7776, -5.674, -5.7559, -5.9785, -4.8534, -5.7335, -5.61, -5.4765, -5.8889, -5.7477, -5.6966, -5.7259, -5.7909, -5.8195, -3.8516, -3.3157, -4.5464, -4.4854, -4.6488, -5.004, -4.8326, -5.1836, -4.3142, -5.3675, -5.3664, -4.5218, -4.521, -5.5894, -5.5894, -5.8292, -5.9447, -3.7478, -6.0296, -6.0809, -6.0809, -6.0809, -4.5761, -6.1766, -6.1766, -6.1766, -5.1015, -6.2813, -4.4224, -6.2818, -5.456, -5.4803, -4.3501, -5.3632, -5.0293, -3.2997, -5.4603, -4.1785, -5.5202, -3.6314, -3.6992, -5.0276, -4.8489, -4.9563, -4.8437, -4.9395, -5.1496, -5.2647, -7.0642, -6.8242, -6.2694, -6.938, -6.9486, -7.3682, -6.9029, -7.0915, -6.7629, -7.1674, -6.6428, -7.4938, -6.0544, -7.6102, -7.6549, -7.6357, -7.6551, -7.6616, -7.6841, -7.6235, -7.6425, -7.6233, -7.6299, -7.0826, -7.2286, -6.9825, -7.6239, -7.5647, -7.4952, -7.1963, -5.6347, -6.3398, -7.0479, -6.9259, -6.911, -5.5238, -7.2185, -7.2192, -7.1693, -7.3646, -4.6895, -5.3075, -5.4033, -6.4172, -7.0314, -5.1149, -5.6909, -6.6664, -6.6142, -6.3399, -6.6343, -6.2995, -5.4265, -6.0306, -3.4345, -5.261, -5.8342, -5.535, -5.6411, -5.5552, -5.3091, -6.0776, -6.4332, -5.9186, -6.1527, -6.0965, -6.068, -5.4049, -6.2234, -6.1073, -5.898, -5.7355, -6.1026, -6.05, -5.7781, -5.7732, -5.9552, -6.0963, -6.0229, -5.7572, -4.9442, -6.1452, -5.4406, -5.3092, -6.4006, -6.543, -5.8214, -6.506, -6.3884, -6.5768, -6.8049, -6.7952, -6.805, -6.805, -5.9909, -6.3563, -6.1079, -6.1231, -6.7858, -7.0916, -6.7972, -7.0915, -7.0799, -6.4102, -5.4951, -7.0928, -6.5875, -7.0953, -7.3159, -5.6365, -6.5754, -6.7879, -6.5283, -6.7397, -6.1117, -5.7405, -6.288, -5.2476, -6.4617, -6.2283, -4.4231, -5.7368, -6.0749, -4.4669, -5.8512, -4.5749, -3.5469, -5.6287, -5.1229, -5.7925, -5.3775, -4.9705, -5.0517, -4.9912, -5.3734, -5.741, -5.3554, -5.553, -5.6953, -5.3223, -5.7574, -5.5113, -5.6048, -5.7667, -5.7854, -5.1325, -6.4059, -6.3415, -6.7323, -6.6831, -6.803, -6.8796, -5.3437, -6.7112, -6.9064, -6.9144, -7.1131, -6.588, -7.107, -7.1203, -7.1581, -7.1581, -7.1216, -7.1157, -7.098, -7.3611, -7.3854, -7.3699, -7.3539, -7.4236, -7.5066, -7.5171, -7.3919, -6.9114, -7.4291, -6.7192, -6.7003, -5.3387, -5.755, -6.5855, -5.4025, -6.3134, -6.9475, -7.169, -5.7638, -5.3287, -5.5309, -4.5083, -5.4073, -4.8018, -6.2093, -6.7925, -6.432, -6.5734, -6.502, -4.7991, -6.3687, -5.1221, -5.1742, -5.5129, -6.3663, -5.5153, -5.8953, -6.0172, -6.0115, -4.9733, -6.0085, -5.8085, -5.9845, -5.6942, -5.9601, -6.0384, -5.7428, -4.3455, -5.6774, -5.5217, -5.9099, -5.6084, -5.5576, -5.5045, -5.8976, -5.6852, -5.9068], \"loglift\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, 2.8133, 2.8026, 2.8019, 2.7998, 2.7995, 2.7983, 2.797, 2.7958, 2.7958, 2.7933, 2.7917, 2.7917, 2.7894, 2.7892, 2.7881, 2.7867, 2.7844, 2.7844, 2.7844, 2.7786, 2.7739, 2.7695, 2.7658, 2.7599, 2.7597, 2.7597, 2.7588, 2.7584, 2.758, 2.758, 2.754, 2.7545, 2.7574, 2.7333, 2.7152, 2.7322, 2.7362, 2.652, 2.6671, 2.6739, 2.6543, 2.5086, 2.4867, 2.4889, 2.5725, 1.9503, 1.9685, 2.369, 1.9242, 1.8294, 1.7276, 1.4028, 1.183, 1.0291, 0.8003, 2.2895, 1.771, 0.3826, 1.2257, 1.2877, 1.0649, 1.2038, 1.6818, -1.1797, 1.0387, 0.7041, 0.2622, 1.3708, 0.5907, 0.2922, -0.1134, 0.3217, 0.5841, 3.272, 3.2702, 3.2696, 3.2694, 3.2674, 3.2667, 3.2655, 3.2654, 3.263, 3.2624, 3.2623, 3.2614, 3.2598, 3.2583, 3.2583, 3.2534, 3.2515, 3.2514, 3.251, 3.2506, 3.2506, 3.2506, 3.2505, 3.2462, 3.2462, 3.2462, 3.2453, 3.2451, 3.2438, 3.2431, 3.2426, 3.2332, 3.1961, 3.2221, 3.2065, 3.1039, 3.2064, 3.0338, 3.1826, 2.8154, 2.7703, 3.0549, 2.9589, 2.9228, 2.7358, 2.3954, 2.1651, 1.5072, 0.4575, 0.457, 0.4569, 0.4568, 0.4563, 0.4563, 0.4557, 0.4557, 0.4554, 0.4553, 0.4552, 0.4552, 0.455, 0.455, 0.4549, 0.4548, 0.4548, 0.4548, 0.4546, 0.4546, 0.4546, 0.4546, 0.4546, 0.4545, 0.4544, 0.4544, 0.4543, 0.4542, 0.4541, 0.4539, 0.4518, 0.4521, 0.4538, 0.452, 0.4515, 0.4367, 0.4531, 0.4528, 0.4511, 0.4524, 0.4207, 0.4267, 0.424, 0.4421, 0.4476, 0.4061, 0.4156, 0.4399, 0.4363, 0.4268, 0.4354, 0.421, 0.3771, 0.4052, 0.2392, 0.3514, 0.385, 0.3412, 0.3477, 0.3319, 0.3034, 0.3773, 0.405, 0.3308, 0.3603, 0.3438, 0.3384, 0.2128, 0.359, 0.2617, 0.1392, 0.0367, 0.2198, 0.1615, -0.1959, -0.2474, -0.0083, 0.2005, -0.1347, 2.7052, 2.7048, 2.7047, 2.7032, 2.7018, 2.6997, 2.6986, 2.698, 2.6948, 2.6931, 2.6926, 2.6916, 2.6914, 2.6906, 2.6906, 2.6901, 2.6897, 2.6889, 2.6835, 2.6832, 2.6829, 2.6829, 2.6827, 2.6798, 2.6791, 2.679, 2.6767, 2.6763, 2.6742, 2.6729, 2.6703, 2.6704, 2.6688, 2.6606, 2.6652, 2.6217, 2.5673, 2.6178, 2.4148, 2.6239, 2.5095, 1.6322, 2.2113, 2.3508, 1.3387, 2.1653, 1.0073, 0.1268, 1.8665, 1.2931, 1.9583, 1.4325, 0.9178, 0.9884, 0.7475, 1.2106, 1.7107, 0.8031, 1.1421, 1.4139, 0.2035, 1.5688, 0.1065, 0.0076, 1.3608, 1.4777, 1.5773, 1.5758, 1.5752, 1.5738, 1.5737, 1.5732, 1.5727, 1.5724, 1.5723, 1.572, 1.572, 1.5705, 1.5705, 1.5704, 1.5703, 1.57, 1.57, 1.5697, 1.5696, 1.5693, 1.5677, 1.5674, 1.5672, 1.567, 1.5663, 1.5651, 1.5651, 1.5645, 1.5642, 1.5642, 1.5609, 1.5511, 1.5069, 1.5102, 1.5343, 1.4679, 1.5147, 1.5462, 1.5556, 1.4451, 1.4066, 1.3883, 1.2304, 1.337, 1.2382, 1.417, 1.5114, 1.435, 1.4615, 1.4421, 1.0065, 1.3972, 0.9905, 0.9843, 1.0711, 1.384, 1.0152, 1.1516, 1.1962, 1.1764, 0.5525, 1.1456, 1.0028, 1.1161, 0.8994, 1.0553, 1.1019, 0.8266, -0.6718, 0.7386, 0.5552, 0.9464, 0.3385, 0.2145, 0.0777, 0.8831, 0.2031, 0.1304]}, \"token.table\": {\"Topic\": [3, 4, 3, 5, 5, 1, 3, 5, 3, 4, 3, 3, 4, 5, 3, 5, 2, 3, 5, 1, 3, 4, 5, 2, 3, 5, 3, 5, 3, 5, 3, 1, 3, 4, 5, 1, 3, 1, 2, 3, 4, 5, 4, 3, 5, 3, 1, 2, 3, 4, 5, 5, 3, 4, 5, 3, 4, 3, 5, 2, 3, 1, 3, 4, 5, 1, 3, 4, 5, 3, 5, 2, 5, 2, 3, 5, 2, 3, 5, 3, 4, 5, 3, 4, 3, 1, 3, 5, 3, 5, 3, 4, 3, 5, 1, 2, 3, 4, 5, 2, 2, 3, 1, 2, 3, 4, 5, 3, 5, 3, 5, 4, 3, 5, 1, 2, 3, 4, 5, 1, 5, 2, 3, 4, 5, 1, 2, 3, 5, 2, 1, 3, 4, 5, 3, 3, 4, 5, 3, 4, 5, 3, 2, 3, 4, 5, 1, 3, 4, 5, 2, 1, 2, 3, 4, 5, 1, 3, 4, 5, 3, 4, 5, 3, 5, 2, 3, 4, 5, 2, 1, 3, 3, 4, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 3, 4, 5, 3, 3, 4, 3, 5, 1, 3, 4, 5, 5, 3, 1, 3, 4, 5, 3, 4, 5, 1, 3, 1, 3, 4, 5, 3, 1, 2, 3, 4, 5, 3, 5, 2, 4, 5, 3, 4, 3, 1, 2, 3, 4, 5, 1, 1, 1, 3, 4, 5, 1, 2, 3, 4, 5, 5, 1, 3, 4, 5, 3, 4, 5, 3, 4, 4, 2, 3, 4, 5, 3, 5, 4, 5, 5, 1, 3, 5, 3, 5, 1, 3, 4, 5, 1, 3, 5, 3, 1, 2, 3, 5, 1, 2, 3, 5, 1, 3, 5, 5, 1, 3, 2, 1, 1, 3, 4, 1, 2, 3, 4, 5, 1, 3, 5, 1, 3, 4, 5, 1, 1, 3, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 2, 3, 1, 4, 1, 2, 3, 4, 5, 3, 4, 1, 3, 4, 5, 3, 4, 5, 3, 5, 1, 5, 2, 3, 1, 2, 3, 4, 5, 1, 2, 3, 5, 1, 3, 4, 5, 5, 2, 3, 5, 2, 3, 1, 2, 3, 4, 5, 1, 2, 3, 5, 2, 3, 3, 4, 1, 3, 5, 3, 4, 5, 5, 3, 5, 3, 5, 1, 2, 3, 4, 5, 1, 1, 2, 3, 4, 5, 3, 4, 5, 1, 3, 4, 5, 1, 3, 3, 5, 5, 1, 2, 3, 4, 5, 3, 4, 5, 3, 5, 5, 1, 3, 5, 3, 5, 1, 3, 4, 5, 1, 2, 3, 4, 5, 2, 5, 1, 3, 4, 5, 1, 1, 5, 3, 4, 5, 1, 3, 4, 5, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 3, 4, 5, 1, 2, 3, 4, 5, 1, 3, 4, 5, 1, 2, 3, 4, 5, 1, 3, 5, 1, 1, 3, 4, 5, 1, 3, 5, 1, 2, 3, 4, 5, 1, 2, 3, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 3, 5, 2, 3, 1, 2, 3, 5, 5, 1, 2, 3, 4, 5, 3, 3, 4, 1, 2, 3, 4, 5, 1, 3, 5, 1, 3, 4, 5, 2, 1, 2, 3, 4, 5, 1, 3, 1, 2, 3, 4, 5, 1, 3, 4, 5, 3, 4, 4, 4, 5, 3, 5, 3, 4, 1, 3, 1, 3, 5, 3, 5, 1, 3, 4, 5, 1, 3, 5, 1, 2, 3, 4, 5, 1, 3, 4, 5, 3, 1, 3, 4, 5, 1, 3, 4, 5, 3, 4, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 3, 5, 3, 5, 3, 4, 1, 2, 3, 4, 5, 3, 4, 5, 2, 1, 3, 4, 3, 5, 4, 5, 3, 3, 5, 1, 3, 4, 5, 3, 5, 1, 3, 3, 4, 5, 1, 2, 3, 5, 2, 3, 4, 1, 3, 4, 5, 3, 4, 5, 3, 5, 1, 3, 4, 5, 1, 3, 4, 5, 3, 4, 2, 2, 5, 3, 4, 5, 1, 2, 3, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 3, 5, 3, 4, 5, 1, 2, 3, 4, 5, 3, 4, 5, 1, 3, 5, 1, 5, 3, 5, 1, 2, 3, 5, 2, 3, 1, 1, 4, 5, 1, 2, 3, 4, 5, 2, 1, 2, 3, 4, 5, 2, 1, 1, 3, 4, 5, 1, 2, 3, 5, 2, 1, 2, 3, 5, 3, 5, 2, 3, 5, 1, 3, 4, 5, 1, 2, 3, 4, 5, 3, 4, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 3, 2, 3, 5, 1, 2, 3, 4, 5, 3, 5, 1, 2, 3, 4, 5, 5, 1, 2, 3, 4, 5, 5, 3, 4, 1, 2, 3, 4, 5, 1, 2, 3, 5, 1, 1, 1, 5, 1, 3, 4, 2, 3, 3, 4, 5, 3, 1, 2, 3, 5, 3, 5, 1, 2, 3, 4, 5, 1, 2, 1, 2, 3, 5, 1, 3, 5], \"Freq\": [0.010730774236584693, 0.9872312297657917, 0.9894367816849321, 0.004623536363013702, 0.9848139949565754, 0.9637856988574939, 0.01302413106564181, 0.01302413106564181, 0.9938331178146692, 0.9657298142056928, 0.9954385685048235, 0.34363558618768814, 0.02749084689501505, 0.6267913092063432, 0.16577701042908974, 0.8342326976431613, 0.9824557971562956, 0.010855865161948017, 0.005427932580974008, 0.031323089139924216, 0.7251769728152152, 0.008542660674524786, 0.2344485762897358, 0.9143066210833738, 0.08187820487313795, 0.013646367478856326, 0.9964496848460664, 0.0018907963659318147, 0.016972592233436317, 0.9844103495393064, 0.9954891618183895, 0.00615525872720365, 0.1600367269072949, 0.15388146818009124, 0.6801560893560034, 0.9760694361942526, 0.03148611084497589, 0.20238445802746638, 0.061905834220166184, 0.3261961264677987, 0.09047775770639672, 0.3190531455962411, 0.9823706842220362, 0.001859814674261435, 0.9968606654041292, 0.9916767375810914, 0.08949651235386566, 0.02105800290679192, 0.01579350218009394, 0.0526450072669798, 0.8212621133648849, 0.9832030493598588, 0.22756597403506393, 0.6930418300158765, 0.08275126328547779, 0.02030799615449004, 0.974783815415522, 0.006217525925287875, 0.99480414804606, 0.9834262641596527, 0.02235059691271938, 0.01722837942243737, 0.7715852784191592, 0.08614189711218684, 0.12552105007775796, 0.9816707054170715, 0.07066546467425686, 0.8055862972865282, 0.11306474347881097, 0.9755371547386643, 0.021323216497019983, 0.9846927676585604, 0.9841554284748083, 0.7304495441061243, 0.14497471867755138, 0.12267091580408195, 0.968043169958811, 0.01936086339917622, 0.00968043169958811, 0.040019523448355084, 0.9004392775879894, 0.06002928517253262, 0.04697326310496987, 0.9629518936518824, 0.9979267818729184, 0.0020347458351483985, 0.9563305425197473, 0.041712289620542166, 0.9955211973426017, 0.004763259317428716, 0.02245314632869908, 0.9654852921340603, 0.8857076166476546, 0.11397477857636486, 0.06626161599994267, 0.006464547902433431, 0.41211492878013123, 0.046867972292642376, 0.4702958599020321, 0.989585892227218, 0.988388447276587, 0.012671646759956244, 0.013622418985783796, 0.33034366040525703, 0.534679945192014, 0.09876253764693252, 0.02383923322512164, 0.9956428975088398, 0.00642350256457316, 0.9954085158473336, 0.004253882546356127, 0.9561364754122488, 0.010873532025048822, 0.978617882254394, 0.009222188198345204, 0.7838859968593423, 0.14755501117352327, 0.027666564595035612, 0.03381469006059908, 0.9917590635905501, 0.00901599148718682, 0.5813711775120256, 0.3727876252748867, 0.035503583359513015, 0.008875895839878254, 0.3335966346862907, 0.4135208284132145, 0.22239775645752716, 0.031274684501839754, 0.9728775175565854, 0.0003756457223029731, 0.9616530490956111, 0.016152766059027844, 0.02178745189357244, 0.9937677362795665, 0.92809949710689, 0.0034163171672646256, 0.06832634334529251, 0.0909698417428909, 0.020993040402205593, 0.8887053770267034, 0.9954274513564499, 0.01880017668081981, 0.9625690460579742, 0.015040141344655847, 0.001880017668081981, 0.08398479948206282, 0.23472674727038068, 0.15289540418529385, 0.5297502736560885, 0.9723686762055233, 0.008700975512462189, 0.029003251708207296, 0.2262253633240169, 0.09281040546626335, 0.6467725130930227, 0.031079391324601316, 0.5520681353712076, 0.16521150125182804, 0.25190664547308433, 0.4179527645449331, 0.25698447009181696, 0.32476059407207636, 0.9942392266845022, 0.004921976369725258, 0.2885361609074859, 0.1236583546746368, 0.5770723218149718, 0.013739817186070756, 0.9780637534014618, 0.9153245715244923, 0.08658475676583036, 0.026703039484683434, 0.9746609411909453, 0.03850323944046598, 0.007700647888093195, 0.1366865000136542, 0.03465291549641938, 0.7835409226134826, 0.13155750675469022, 0.06104268313417627, 0.6304235723684756, 0.010524600540375219, 0.16628868853792844, 0.01631653650730781, 0.3181724618925023, 0.42422994919000306, 0.2406689134827902, 0.9961368520675409, 0.0396544981619746, 0.9517079558873903, 0.9962063262653927, 0.004633517796583222, 0.24098409927472714, 0.708283700477024, 0.0020955139067367575, 0.04819681985494543, 0.9906522396616966, 0.9979251520345905, 0.0007011305136248686, 0.9675601088023187, 0.03014861208586935, 0.002103391540874606, 0.018368905893731566, 0.9551831064740414, 0.02755335884059735, 0.953649829031034, 0.038145993161241364, 0.9062529941076602, 0.093734340786002, 0.36322057054575774, 0.5428780570522616, 0.9951063334142738, 0.03854328425753802, 0.039747761890586084, 0.5191298598437153, 0.18006940614068545, 0.22222612329736766, 0.9927261195437975, 0.004866304507567635, 0.9812597698374822, 0.9765486836622427, 0.01992956497269883, 0.030151972941432254, 0.9648631341258321, 0.9979461659812149, 0.4169646024230177, 0.07044607438197102, 0.22466585883979948, 0.051406594819276154, 0.2341855986211469, 0.9617970687092177, 0.9849359108362166, 0.03306246389347153, 0.9204890515796053, 0.0007514196339425349, 0.04583659767049463, 0.008689555049503845, 0.006827507538895878, 0.8968862176095039, 0.06641302787835081, 0.02172388762375961, 0.9922340170313713, 0.018219780786778626, 0.8018489799200906, 0.07475468881634172, 0.10512099012763942, 0.0512114631681348, 0.011380325148474401, 0.9331866621749009, 0.02404917598438246, 0.9619670393752985, 0.9861361377502644, 0.023664501994653905, 0.10649025897594258, 0.005916125498663476, 0.8637543228048675, 0.9969899263348233, 0.9772349860687187, 0.007918554194364155, 0.9898192742955193, 0.9852923483773204, 0.8413352185242858, 0.15144033933437145, 0.008413352185242858, 0.9927905071827451, 0.003848025221638547, 0.002293007905276554, 0.08713430040050904, 0.08484129249523249, 0.8254828458995593, 0.96750227354382, 0.10482576425685383, 0.8932028662719421, 0.9934839109894461, 0.4065184966550816, 0.029641973714433036, 0.3387654138792347, 0.22019751902150253, 0.9388368190298645, 0.9463716873347218, 0.03639891105133545, 0.01213297035044515, 0.8357901877463905, 0.11143869169951873, 0.03714623056650625, 0.9877460396936872, 0.932922254734025, 0.06219481698226833, 0.9626381452364878, 0.9200608869473303, 0.9400940781768826, 0.007803500181460216, 0.9832410228639873, 0.21493862394387253, 0.03714988561992858, 0.27597057889089804, 0.11144965685978575, 0.3608846031650205, 0.7165544399605451, 0.24236400175136086, 0.04215026117414972, 0.0014141709803788247, 0.9460803858734338, 0.011313367843030598, 0.039596787450607095, 0.9062587666560721, 0.7135490518042598, 0.16680367444774905, 0.11120244963183269, 0.04770013240274557, 0.000867275134595374, 0.62617264717786, 0.03729283078760108, 0.28880261982025957, 0.1663117201417228, 0.027214645114100093, 0.48230398841099614, 0.042333892399711255, 0.28272992424092874, 0.9772719410158154, 0.022727254442228263, 0.9781673269723773, 0.9424544233687825, 0.0596173699189153, 0.006167314129542962, 0.6105640988247533, 0.275473364452919, 0.0493385130363437, 0.030151972941432254, 0.9648631341258321, 0.10647336088854394, 0.7594244535664821, 0.01154530419273368, 0.12186709981218885, 0.020084707818991746, 0.9640659753116038, 0.020084707818991746, 0.02606747136926121, 0.9731855977857519, 0.9330377007818759, 0.05183542782121532, 0.9873692672774634, 0.008855329751367385, 0.16825781830177658, 0.015836029957814267, 0.6453182207809314, 0.07126213481016419, 0.10095469098106594, 0.7324539419721635, 0.037561740613957104, 0.13146609214884986, 0.10329478668838203, 0.0347967688573475, 0.4146614955500577, 0.3363687656210258, 0.21264692079490138, 0.9953431368903097, 0.9649129815466297, 0.013590323683755349, 0.013590323683755349, 0.994276166405069, 0.006214226040031681, 0.3690826182199469, 0.028706425861551428, 0.09022019556487593, 0.004100917980221633, 0.5044129115672609, 0.10680980414093957, 0.8421542249574082, 0.04929683268043365, 0.0013693564633453792, 0.9950646026072895, 0.005621833913035533, 0.04437813933935722, 0.9319409261265016, 0.002764818855127429, 0.9815106935702373, 0.016588913130764573, 0.07965352827262279, 0.049017555860075564, 0.873125213757596, 0.9813086263927786, 0.9753277831088716, 0.023533378144718082, 0.9953934449098439, 0.002742130702230975, 0.001637400058123588, 0.006549600232494352, 0.9038448320842205, 0.044209801569336876, 0.044209801569336876, 0.9849126038887954, 0.002773101427026013, 0.011092405708104052, 0.1941170998918209, 0.2717639398485493, 0.5241161697079164, 0.032553392797078404, 0.11626211713242288, 0.8463882127240385, 0.17234504410119506, 0.39639360143274865, 0.13787603528095604, 0.29490151990648933, 0.95260921172978, 0.03969205048874083, 0.9904102645480688, 0.007255752853832006, 0.9879320228757175, 0.08188583464996567, 0.008188583464996567, 0.2763646919436341, 0.07881511585059195, 0.5537529568203928, 0.029000655674066362, 0.95702163724419, 0.9923449630590948, 0.9936068224957042, 0.0041058133160979515, 0.9959360112488612, 0.7760076984751725, 0.15872884741537618, 0.0529096158051254, 0.9913429429362428, 0.006057085191056473, 0.08550702815136985, 0.24252902530206719, 0.10416310702075962, 0.5674557322772725, 0.1201969435084946, 0.027545132887363346, 0.5120890614059822, 0.05383821427984654, 0.2867197923275548, 0.9639556496720523, 0.9921818882356115, 0.08427613409730507, 0.4055788953432806, 0.46878599591625947, 0.0474053254297341, 0.9827664850629718, 0.9632278667629917, 0.038529114670519664, 0.07355117839556133, 0.8550324488484004, 0.06435728109611616, 0.027777102399191317, 0.820085880357077, 0.03174525988479008, 0.11904472456796278, 0.9867640130482639, 0.015250230687379936, 0.05406899970980159, 0.26064030629340257, 0.23984453717424808, 0.4311656130704691, 0.026710577956213732, 0.013355288978106866, 0.08585542914497271, 0.032434273232545247, 0.8394753071952887, 0.19295450476841997, 0.0033557305177116512, 0.5721520532698365, 0.01174505681199078, 0.21980034891011316, 0.0610800507658792, 0.8906323681443316, 0.036932123718903705, 0.011363730375047293, 0.004229673043865499, 0.008459346087730997, 0.9474467618258717, 0.019033528697394744, 0.021148365219327492, 0.12279544414731447, 0.40870722455001685, 0.20710276400964978, 0.2639185665255714, 0.009630104460665064, 0.008497150994704468, 0.9477155742760384, 0.010196581193645362, 0.02379202278517251, 0.011071558001215521, 0.022143116002431042, 0.9632255461057504, 0.96750227354382, 0.06518368429742538, 0.5491231586267956, 0.026666052667128567, 0.3585102636358396, 0.8592248605768693, 0.12729257193731397, 0.015911571492164246, 0.07676448711844844, 0.015493749693631795, 0.06479204417336933, 0.13873948589297563, 0.7042613497105362, 0.0009972008933436566, 0.003988803573374626, 0.9922148888769383, 0.003988803573374626, 0.015147723883453784, 0.01009848258896919, 0.1943957898376569, 0.1893465485431723, 0.5907612314546976, 0.0014625676000662804, 0.6025778512273076, 0.3802675760172329, 0.011700540800530243, 0.002925135200132561, 0.9955017628042641, 0.004566521847725982, 0.991399346077344, 0.007454130421634166, 0.007339214689839719, 0.7999744011925294, 0.15412350848663411, 0.029356858759358877, 0.9819812632081302, 0.002859703963669399, 0.6305647239891025, 0.18016134971117212, 0.09008067485558606, 0.09580008278292486, 0.9937206839834402, 0.02391265546012924, 0.9565062184051696, 0.05276531173657981, 0.011794599094059017, 0.854798050132593, 0.018002282827774288, 0.062076837337152725, 0.9217673583324223, 0.03687069433329689, 0.03687069433329689, 0.13288413057826404, 0.15112312889292773, 0.06513927969522747, 0.6513927969522747, 0.9812597698374822, 0.05434935446988155, 0.011441969362080327, 0.314654157457209, 0.002860492340520082, 0.6178663455523377, 0.8639252766348199, 0.11780799226838452, 0.04293940116165399, 0.012429826652057734, 0.7423996464001756, 0.028249606027403944, 0.17514755736990445, 0.06949092810819656, 0.043788804013384135, 0.17705907709759672, 0.710140169434447, 0.014271936556773257, 0.9847636224173547, 0.980235267672102, 0.9564648580996931, 0.9936922838529945, 0.9968265798800751, 0.00429666629258653, 0.029688670211700043, 0.9500374467744014, 0.9606483523362352, 0.027447095781035293, 0.9644843007210838, 0.0688917357657917, 0.9847499693326971, 0.06410553688613851, 0.9341092517694468, 0.001441393246953328, 0.9210502848031766, 0.02017950545734659, 0.057655729878133125, 0.3485070579299532, 0.10723294090152406, 0.5415263515526965, 0.03984547308642389, 0.02903027324868026, 0.49294542418347265, 0.08082938826103131, 0.35747081568963146, 0.018072907967868476, 0.16717439870278342, 0.2515146358861696, 0.5632722983319008, 0.9956627963910909, 0.09056180621549248, 0.38488767641584304, 0.2878571697563868, 0.23610756620467682, 0.019606379465679395, 0.8798362785223628, 0.03512809654267558, 0.06535459821893132, 0.029362082195849126, 0.9689487124630212, 0.005050310197202922, 0.02777670608461607, 0.7865858132143551, 0.06944176521154018, 0.11110682433846428, 0.19820235156216143, 0.04059566236815355, 0.45849218674620473, 0.05253556306466929, 0.25312589476613384, 0.9963338653687327, 0.00355833523345976, 0.017883916809148716, 0.9836154245031793, 0.011359234298629228, 0.9768941496821136, 0.31754909377396806, 0.009622699811332366, 0.36085124292496373, 0.10584969792465602, 0.20688804594364588, 0.019445498744247502, 0.048613746860618756, 0.9301430232665056, 0.9723686762055233, 0.9692907059228342, 0.08359390276182922, 0.9028141498277555, 0.9904620888785124, 0.0056276255049915475, 0.9617132731636223, 0.019234265463272445, 0.9926724359282205, 0.9873279921367517, 0.012040585269960387, 0.42445820149653213, 0.3013904389324489, 0.1984153722971955, 0.07534760973311222, 0.99582861251405, 0.0030926354425902173, 0.9489456168652622, 0.06326304112435081, 0.019171278603844596, 0.10863724542178603, 0.8690979633742882, 0.007915989029291472, 0.17019376412976664, 0.5917201799395375, 0.2295636818494527, 0.9517751084669251, 0.027587684303389134, 0.9517702660393096, 0.0008790695632353836, 0.9766462847545112, 0.015823252138236904, 0.006153486942647685, 0.024127814624860747, 0.9570699801194764, 0.008042604874953584, 0.008972427326256062, 0.9869670058881669, 0.002130349380335363, 0.05751943326905481, 0.01278209628201218, 0.9288323298262184, 0.06750166001068901, 0.16875415002672253, 0.7376967129739586, 0.028929282861723863, 0.014990608649809074, 0.9743895622375898, 0.9921965086525559, 0.9847150839826976, 0.010880829657267376, 0.027538440087356966, 0.9363069629701368, 0.027538440087356966, 0.005987844701451654, 0.7005778300698434, 0.2814287009682277, 0.011975689402903307, 0.09934566361175819, 0.0711977255884267, 0.3030042740158625, 0.019869132722351638, 0.5050071233597708, 0.23207992661780327, 0.051573317026178504, 0.23530325893193943, 0.11603996330890164, 0.3610132191832496, 0.6318940991471932, 0.2570416674497057, 0.1071006947707107, 0.07035210441959439, 0.900506936570808, 0.014070420883918876, 0.012928855098554516, 0.02343354986613006, 0.8880507345819634, 0.02343354986613006, 0.051715420394218066, 0.015139669755888143, 0.9689388643768412, 0.030279339511776287, 0.027564995691740517, 0.9667437774746139, 0.003937856527391503, 0.8924112153208094, 0.09493736333200099, 0.9930909004223949, 0.004868092649129387, 0.008555472405734753, 0.932546492225088, 0.042777362028673764, 0.017110944811469506, 0.994258239817651, 0.005288607658604527, 0.9666757534458263, 0.9141781340131141, 0.019045377791939877, 0.05713613337581963, 0.01680467865500613, 0.010693886416822083, 0.905924949310785, 0.013749282535914106, 0.054997130143656425, 0.9812597698374822, 0.0007694810607414091, 0.005386367425189864, 0.9649292501697271, 0.0007694810607414091, 0.02847079924743214, 0.9723671668837247, 0.96750227354382, 0.9506428666283845, 0.03802571466513538, 0.013947588310160132, 0.9763311817112093, 0.01316253634715169, 0.93454008064777, 0.02632507269430338, 0.02632507269430338, 0.9780637534014618, 0.012877957850697732, 0.9229203126333374, 0.04292652616899244, 0.02146326308449622, 0.010991117622770028, 0.9892005860493026, 0.9940568082209903, 0.0032861382089950093, 0.0016430691044975047, 0.003280098352040538, 0.1754852618341688, 0.2214066387627363, 0.6002579984234184, 0.046082811105310116, 0.010716932815188398, 0.24541776146781433, 0.14682197956808107, 0.5508503467006837, 0.013665396662560898, 0.9839085597043846, 0.15026699748466996, 0.014409164142365611, 0.17290996970838735, 0.10292260101689722, 0.5598989495319209, 0.08674204068275693, 0.008736608414090626, 0.7813024096029617, 0.07301308460347167, 0.050547520110095764, 0.9478496926422789, 0.030575796536847707, 0.9762363869411067, 0.02235655847956733, 0.0024840620532852586, 0.010561648612591297, 0.021123297225182594, 0.878963867870098, 0.019949780712672453, 0.0692374742380985, 0.9931113734288172, 0.007144686139775663, 0.005999597642092992, 0.011999195284185984, 0.27598149153627766, 0.03899738467360445, 0.6659553382723221, 0.9844671693644961, 0.007618478918580814, 0.030473915674323258, 0.007618478918580814, 0.007618478918580814, 0.9523098648226018, 0.9912207937970711, 0.994495205911661, 0.9381259906278898, 0.011979451942771198, 0.007986301295180798, 0.4292636946159679, 0.05390753374247039, 0.49714725562500467, 0.962643515767014, 0.977640038652869, 0.019951837523527936, 0.9942876672227413, 0.9807726055226138, 0.9807726055226138, 0.970895245314275, 0.9937160472090443, 0.08340745532327687, 0.3143819469877359, 0.6031000615683098, 0.9959005186678088, 0.0028132783013214936, 0.5098446991295879, 0.3169304886481222, 0.1722448307870229, 0.9913516785711776, 0.5862050748789198, 0.03589010662523998, 0.3110475907520799, 0.07178021325047997, 0.006858144863158858, 0.9921449568703148, 0.04296567520730393, 0.021118721712064643, 0.654680373074004, 0.025488112411112502, 0.254881124111125, 0.0191173261808219, 0.9654249721315059, 0.07958414903779885, 0.001808730659949974, 0.8935129460152871, 0.02441786390932465, 0.00431917665981211, 0.9826126901072552, 0.012957529979436331], \"Term\": [\"aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa\", \"aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa\", \"accounting\", \"accounting\", \"addin\", \"adult\", \"adult\", \"adult\", \"afford\", \"agave\", \"aggressively\", \"agreement\", \"agreement\", \"agreement\", \"alert\", \"alert\", \"align\", \"align\", \"align\", \"also\", \"also\", \"also\", \"also\", \"amazon\", \"amazon\", \"amazon\", \"asset\", \"asset\", \"assign\", \"assign\", \"astronomical\", \"attached\", \"attached\", \"attached\", \"attached\", \"auth\", \"auth\", \"available\", \"available\", \"available\", \"available\", \"available\", \"backout\", \"bakernet\", \"bakernet\", \"bankrupt\", \"barry\", \"barry\", \"barry\", \"barry\", \"barry\", \"baughman\", \"ben\", \"ben\", \"ben\", \"bergfelt\", \"bergfelt\", \"beth\", \"beth\", \"bgcolor\", \"bgcolor\", \"bill\", \"bill\", \"bill\", \"bill\", \"blah\", \"blair\", \"blair\", \"blair\", \"board\", \"board\", \"bodydefault\", \"bolt\", \"border\", \"border\", \"border\", \"br\", \"br\", \"br\", \"bradford\", \"bradford\", \"bradford\", \"breese\", \"breese\", \"brochure\", \"business\", \"business\", \"business\", \"buying\", \"buying\", \"caiso\", \"caiso\", \"california\", \"california\", \"call\", \"call\", \"call\", \"call\", \"call\", \"cellpadding\", \"cellspacing\", \"cellspacing\", \"center\", \"center\", \"center\", \"center\", \"center\", \"ceo\", \"ceo\", \"chairman\", \"chairman\", \"chanley\", \"cherry\", \"cherry\", \"class\", \"class\", \"class\", \"class\", \"class\", \"classmate\", \"classmate\", \"clear\", \"clear\", \"clear\", \"clear\", \"click\", \"click\", \"click\", \"click\", \"colspan\", \"company\", \"company\", \"company\", \"company\", \"competitive\", \"conference\", \"conference\", \"conference\", \"confidential\", \"confidential\", \"confidential\", \"congestion\", \"consumer\", \"consumer\", \"consumer\", \"consumer\", \"contact\", \"contact\", \"contact\", \"contact\", \"coords\", \"copy\", \"copy\", \"copy\", \"copy\", \"copy\", \"corp\", \"corp\", \"corp\", \"corp\", \"credit\", \"credit\", \"credit\", \"crisis\", \"crisis\", \"ct\", \"ct\", \"ct\", \"ct\", \"ctr\", \"curve\", \"curve\", \"darrell\", \"darrell\", \"date\", \"date\", \"date\", \"date\", \"date\", \"day\", \"day\", \"day\", \"day\", \"day\", \"december\", \"december\", \"december\", \"december\", \"declared\", \"demoes\", \"demoes\", \"design\", \"design\", \"desk\", \"desk\", \"desk\", \"desk\", \"detected\", \"devastated\", \"development\", \"development\", \"development\", \"development\", \"devon\", \"devon\", \"devon\", \"diabetes\", \"diabetes\", \"dickerman\", \"doc\", \"doc\", \"doc\", \"donate\", \"e\", \"e\", \"e\", \"e\", \"e\", \"earnings\", \"earnings\", \"ecar\", \"eei\", \"eei\", \"eix\", \"eix\", \"electricity\", \"email\", \"email\", \"email\", \"email\", \"email\", \"emailed\", \"emaillink\", \"employee\", \"employee\", \"employee\", \"employee\", \"energy\", \"energy\", \"energy\", \"energy\", \"energy\", \"enform\", \"enron\", \"enron\", \"enron\", \"enron\", \"eol\", \"eol\", \"eol\", \"epe\", \"epe\", \"epenergy\", \"error\", \"error\", \"error\", \"error\", \"euci\", \"evelyn\", \"exotica\", \"exotica\", \"fagan\", \"feedback\", \"feedback\", \"feedback\", \"ferc\", \"ferc\", \"file\", \"file\", \"file\", \"file\", \"filet\", \"final\", \"final\", \"financially\", \"find\", \"find\", \"find\", \"find\", \"fish\", \"font\", \"font\", \"font\", \"football\", \"football\", \"football\", \"fran\", \"franklin\", \"franklin\", \"frcc\", \"frenzy\", \"freshman\", \"fri\", \"fri\", \"friday\", \"friday\", \"friday\", \"friday\", \"friday\", \"friend\", \"friend\", \"friend\", \"fund\", \"fund\", \"fund\", \"fund\", \"galore\", \"game\", \"game\", \"game\", \"gas\", \"gas\", \"gas\", \"gas\", \"gas\", \"get\", \"get\", \"get\", \"get\", \"get\", \"gif\", \"gif\", \"gorny\", \"graf\", \"greg\", \"greg\", \"greg\", \"greg\", \"greg\", \"gregwhalley\", \"gregwhalley\", \"group\", \"group\", \"group\", \"group\", \"hanagriff\", \"hanagriff\", \"hanagriff\", \"harrison\", \"harrison\", \"headcount\", \"headcount\", \"height\", \"height\", \"help\", \"help\", \"help\", \"help\", \"help\", \"holiday\", \"holiday\", \"holiday\", \"holiday\", \"hou\", \"hou\", \"hou\", \"hou\", \"hourahead\", \"hp\", \"hp\", \"hp\", \"href\", \"href\", \"id\", \"id\", \"id\", \"id\", \"id\", \"image\", \"image\", \"image\", \"image\", \"img\", \"img\", \"impacted\", \"impacted\", \"industry\", \"industry\", \"industry\", \"intended\", \"intended\", \"intended\", \"interchange\", \"investment\", \"investment\", \"investor\", \"investor\", \"issue\", \"issue\", \"issue\", \"issue\", \"issue\", \"itcapps\", \"january\", \"january\", \"january\", \"january\", \"january\", \"jason\", \"jason\", \"jason\", \"john\", \"john\", \"john\", \"john\", \"jurgens\", \"jurgens\", \"ken\", \"ken\", \"kern\", \"know\", \"know\", \"know\", \"know\", \"know\", \"kowalke\", \"kowalke\", \"krishna\", \"largest\", \"largest\", \"laurel\", \"lavorato\", \"lavorato\", \"lavorato\", \"lay\", \"lay\", \"let\", \"let\", \"let\", \"let\", \"like\", \"like\", \"like\", \"like\", \"like\", \"linkbarseperator\", \"locate\", \"london\", \"london\", \"london\", \"london\", \"lover\", \"lubbock\", \"lubbock\", \"lynn\", \"lynn\", \"lynn\", \"made\", \"made\", \"made\", \"made\", \"maggi\", \"mail\", \"mail\", \"mail\", \"mail\", \"mail\", \"mailto\", \"mailto\", \"mailto\", \"mailto\", \"mailto\", \"make\", \"make\", \"make\", \"make\", \"make\", \"management\", \"management\", \"management\", \"management\", \"many\", \"many\", \"many\", \"many\", \"many\", \"mark\", \"mark\", \"mark\", \"mark\", \"market\", \"market\", \"market\", \"market\", \"market\", \"matt\", \"matt\", \"matt\", \"mawhitt\", \"may\", \"may\", \"may\", \"may\", \"membership\", \"membership\", \"membership\", \"message\", \"message\", \"message\", \"message\", \"message\", \"million\", \"million\", \"million\", \"million\", \"monday\", \"monday\", \"monday\", \"monday\", \"monday\", \"money\", \"money\", \"money\", \"money\", \"money\", \"mr\", \"mr\", \"nbsp\", \"nbsp\", \"ne\", \"ne\", \"ne\", \"ne\", \"nella\", \"net\", \"net\", \"net\", \"net\", \"net\", \"netted\", \"neumin\", \"neumin\", \"new\", \"new\", \"new\", \"new\", \"new\", \"nguyen\", \"nguyen\", \"nguyen\", \"november\", \"november\", \"november\", \"november\", \"npcc\", \"number\", \"number\", \"number\", \"number\", \"number\", \"nytimes\", \"nytimes\", \"one\", \"one\", \"one\", \"one\", \"one\", \"original\", \"original\", \"original\", \"original\", \"outage\", \"outage\", \"pager\", \"pampa\", \"parsing\", \"partnership\", \"partnership\", \"pcg\", \"pcg\", \"pep\", \"pep\", \"perfmgmt\", \"perfmgmt\", \"permian\", \"permit\", \"permit\", \"plan\", \"plan\", \"plan\", \"plan\", \"play\", \"play\", \"play\", \"please\", \"please\", \"please\", \"please\", \"please\", \"pm\", \"pm\", \"pm\", \"pm\", \"pocketbook\", \"point\", \"point\", \"point\", \"point\", \"power\", \"power\", \"power\", \"power\", \"powersrc\", \"powersrc\", \"price\", \"price\", \"price\", \"price\", \"price\", \"product\", \"product\", \"product\", \"product\", \"product\", \"profit\", \"profit\", \"pseg\", \"pseg\", \"pt\", \"pt\", \"received\", \"received\", \"received\", \"received\", \"received\", \"recipient\", \"recipient\", \"recipient\", \"rect\", \"redshirt\", \"ref\", \"ref\", \"regulatory\", \"regulatory\", \"reliantenergy\", \"reliantenergy\", \"repair\", \"reported\", \"reported\", \"request\", \"request\", \"request\", \"request\", \"retirement\", \"retirement\", \"reviewer\", \"reviewer\", \"revised\", \"revised\", \"revised\", \"right\", \"right\", \"right\", \"right\", \"rk\", \"rk\", \"royalty\", \"said\", \"said\", \"said\", \"said\", \"sat\", \"sat\", \"sat\", \"savita\", \"savita\", \"schedule\", \"schedule\", \"schedule\", \"schedule\", \"scheduled\", \"scheduled\", \"scheduled\", \"scheduled\", \"schoolcraft\", \"schoolcraft\", \"scoop\", \"script\", \"script\", \"sdg\", \"sdg\", \"sdg\", \"se\", \"se\", \"se\", \"se\", \"see\", \"see\", \"see\", \"see\", \"see\", \"send\", \"send\", \"send\", \"send\", \"send\", \"sept\", \"sept\", \"sept\", \"server\", \"server\", \"server\", \"service\", \"service\", \"service\", \"service\", \"service\", \"sfs\", \"sfs\", \"sfs\", \"share\", \"share\", \"share\", \"simulation\", \"simulation\", \"sold\", \"sold\", \"sp\", \"sp\", \"sp\", \"sp\", \"src\", \"src\", \"srrs\", \"stack\", \"stack\", \"stack\", \"state\", \"state\", \"state\", \"state\", \"state\", \"std\", \"stock\", \"stock\", \"stock\", \"stock\", \"stock\", \"stocklookup\", \"strick\", \"strict\", \"strict\", \"stsw\", \"stsw\", \"sw\", \"sw\", \"sw\", \"sw\", \"syncrasy\", \"table\", \"table\", \"table\", \"table\", \"tagline\", \"tagline\", \"td\", \"td\", \"td\", \"team\", \"team\", \"team\", \"team\", \"thanks\", \"thanks\", \"thanks\", \"thanks\", \"thanks\", \"thru\", \"thru\", \"thursday\", \"thursday\", \"thursday\", \"thursday\", \"thursday\", \"time\", \"time\", \"time\", \"time\", \"time\", \"todaysheadlines\", \"todaysheadlines\", \"tr\", \"tr\", \"tr\", \"trading\", \"trading\", \"trading\", \"trading\", \"trading\", \"transition\", \"transition\", \"tuesday\", \"tuesday\", \"tuesday\", \"tuesday\", \"tuesday\", \"txt\", \"tycholiz\", \"tycholiz\", \"tycholiz\", \"tycholiz\", \"tycholiz\", \"tyrell\", \"underhanded\", \"unify\", \"use\", \"use\", \"use\", \"use\", \"use\", \"validation\", \"valign\", \"valign\", \"variance\", \"vegetarian\", \"veggie\", \"vladimir\", \"westdesk\", \"whalley\", \"whalley\", \"whalley\", \"width\", \"width\", \"williams\", \"williams\", \"williams\", \"wiped\", \"wish\", \"wish\", \"wish\", \"wish\", \"wj\", \"wj\", \"would\", \"would\", \"would\", \"would\", \"would\", \"wscc\", \"wscc\", \"year\", \"year\", \"year\", \"year\", \"york\", \"york\", \"york\"]}, \"R\": 30, \"lambda.step\": 0.01, \"plot.opts\": {\"xlab\": \"PC1\", \"ylab\": \"PC2\"}, \"topic.order\": [1, 2, 3, 4, 5]};\n",
       "\n",
       "function LDAvis_load_lib(url, callback){\n",
       "  var s = document.createElement('script');\n",
       "  s.src = url;\n",
       "  s.async = true;\n",
       "  s.onreadystatechange = s.onload = callback;\n",
       "  s.onerror = function(){console.warn(\"failed to load library \" + url);};\n",
       "  document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "}\n",
       "\n",
       "if(typeof(LDAvis) !== \"undefined\"){\n",
       "   // already loaded: just create the visualization\n",
       "   !function(LDAvis){\n",
       "       new LDAvis(\"#\" + \"ldavis_el378231401055981753129857172019\", ldavis_el378231401055981753129857172019_data);\n",
       "   }(LDAvis);\n",
       "}else if(typeof define === \"function\" && define.amd){\n",
       "   // require.js is available: use it to load d3/LDAvis\n",
       "   require.config({paths: {d3: \"https://d3js.org/d3.v5\"}});\n",
       "   require([\"d3\"], function(d3){\n",
       "      window.d3 = d3;\n",
       "      LDAvis_load_lib(\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.3.1/pyLDAvis/js/ldavis.v3.0.0.js\", function(){\n",
       "        new LDAvis(\"#\" + \"ldavis_el378231401055981753129857172019\", ldavis_el378231401055981753129857172019_data);\n",
       "      });\n",
       "    });\n",
       "}else{\n",
       "    // require.js not available: dynamically load d3 & LDAvis\n",
       "    LDAvis_load_lib(\"https://d3js.org/d3.v5.js\", function(){\n",
       "         LDAvis_load_lib(\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.3.1/pyLDAvis/js/ldavis.v3.0.0.js\", function(){\n",
       "                 new LDAvis(\"#\" + \"ldavis_el378231401055981753129857172019\", ldavis_el378231401055981753129857172019_data);\n",
       "            })\n",
       "         });\n",
       "}\n",
       "</script>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/masoud/anaconda3/envs/coding_env/lib/python3.7/site-packages/past/builtins/misc.py:45: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
      "  from imp import reload\n",
      "/home/masoud/anaconda3/envs/coding_env/lib/python3.7/site-packages/joblib/backports.py:36: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  if LooseVersion(np.__version__) < '1.13':\n",
      "/home/masoud/anaconda3/envs/coding_env/lib/python3.7/site-packages/setuptools/_distutils/version.py:346: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  other = LooseVersion(other)\n",
      "/home/masoud/anaconda3/envs/coding_env/lib/python3.7/site-packages/joblib/backports.py:36: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  if LooseVersion(np.__version__) < '1.13':\n",
      "/home/masoud/anaconda3/envs/coding_env/lib/python3.7/site-packages/setuptools/_distutils/version.py:346: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  other = LooseVersion(other)\n",
      "/home/masoud/anaconda3/envs/coding_env/lib/python3.7/site-packages/joblib/backports.py:36: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  if LooseVersion(np.__version__) < '1.13':\n",
      "/home/masoud/anaconda3/envs/coding_env/lib/python3.7/site-packages/setuptools/_distutils/version.py:346: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  other = LooseVersion(other)\n",
      "/home/masoud/anaconda3/envs/coding_env/lib/python3.7/site-packages/joblib/backports.py:36: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  if LooseVersion(np.__version__) < '1.13':\n",
      "/home/masoud/anaconda3/envs/coding_env/lib/python3.7/site-packages/setuptools/_distutils/version.py:346: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  other = LooseVersion(other)\n",
      "/home/masoud/anaconda3/envs/coding_env/lib/python3.7/site-packages/past/builtins/misc.py:45: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
      "  from imp import reload\n",
      "/home/masoud/anaconda3/envs/coding_env/lib/python3.7/site-packages/joblib/backports.py:36: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  if LooseVersion(np.__version__) < '1.13':\n",
      "/home/masoud/anaconda3/envs/coding_env/lib/python3.7/site-packages/setuptools/_distutils/version.py:346: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  other = LooseVersion(other)\n",
      "/home/masoud/anaconda3/envs/coding_env/lib/python3.7/site-packages/joblib/backports.py:36: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  if LooseVersion(np.__version__) < '1.13':\n",
      "/home/masoud/anaconda3/envs/coding_env/lib/python3.7/site-packages/setuptools/_distutils/version.py:346: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  other = LooseVersion(other)\n",
      "/home/masoud/anaconda3/envs/coding_env/lib/python3.7/site-packages/joblib/backports.py:36: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  if LooseVersion(np.__version__) < '1.13':\n",
      "/home/masoud/anaconda3/envs/coding_env/lib/python3.7/site-packages/setuptools/_distutils/version.py:346: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  other = LooseVersion(other)\n",
      "/home/masoud/anaconda3/envs/coding_env/lib/python3.7/site-packages/joblib/backports.py:36: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  if LooseVersion(np.__version__) < '1.13':\n",
      "/home/masoud/anaconda3/envs/coding_env/lib/python3.7/site-packages/setuptools/_distutils/version.py:346: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  other = LooseVersion(other)\n",
      "/home/masoud/anaconda3/envs/coding_env/lib/python3.7/site-packages/past/builtins/misc.py:45: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
      "  from imp import reload\n",
      "/home/masoud/anaconda3/envs/coding_env/lib/python3.7/site-packages/joblib/backports.py:36: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  if LooseVersion(np.__version__) < '1.13':\n",
      "/home/masoud/anaconda3/envs/coding_env/lib/python3.7/site-packages/setuptools/_distutils/version.py:346: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  other = LooseVersion(other)\n",
      "/home/masoud/anaconda3/envs/coding_env/lib/python3.7/site-packages/joblib/backports.py:36: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  if LooseVersion(np.__version__) < '1.13':\n",
      "/home/masoud/anaconda3/envs/coding_env/lib/python3.7/site-packages/setuptools/_distutils/version.py:346: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  other = LooseVersion(other)\n",
      "/home/masoud/anaconda3/envs/coding_env/lib/python3.7/site-packages/joblib/backports.py:36: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  if LooseVersion(np.__version__) < '1.13':\n",
      "/home/masoud/anaconda3/envs/coding_env/lib/python3.7/site-packages/setuptools/_distutils/version.py:346: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  other = LooseVersion(other)\n",
      "/home/masoud/anaconda3/envs/coding_env/lib/python3.7/site-packages/joblib/backports.py:36: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  if LooseVersion(np.__version__) < '1.13':\n",
      "/home/masoud/anaconda3/envs/coding_env/lib/python3.7/site-packages/setuptools/_distutils/version.py:346: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  other = LooseVersion(other)\n",
      "/home/masoud/anaconda3/envs/coding_env/lib/python3.7/site-packages/joblib/backports.py:36: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  if LooseVersion(np.__version__) < '1.13':\n",
      "/home/masoud/anaconda3/envs/coding_env/lib/python3.7/site-packages/setuptools/_distutils/version.py:346: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  other = LooseVersion(other)\n",
      "/home/masoud/anaconda3/envs/coding_env/lib/python3.7/site-packages/joblib/backports.py:36: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  if LooseVersion(np.__version__) < '1.13':\n",
      "/home/masoud/anaconda3/envs/coding_env/lib/python3.7/site-packages/setuptools/_distutils/version.py:346: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  other = LooseVersion(other)\n",
      "/home/masoud/anaconda3/envs/coding_env/lib/python3.7/site-packages/past/builtins/misc.py:45: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
      "  from imp import reload\n",
      "/home/masoud/anaconda3/envs/coding_env/lib/python3.7/site-packages/joblib/backports.py:36: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  if LooseVersion(np.__version__) < '1.13':\n",
      "/home/masoud/anaconda3/envs/coding_env/lib/python3.7/site-packages/setuptools/_distutils/version.py:346: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  other = LooseVersion(other)\n",
      "/home/masoud/anaconda3/envs/coding_env/lib/python3.7/site-packages/joblib/backports.py:36: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  if LooseVersion(np.__version__) < '1.13':\n",
      "/home/masoud/anaconda3/envs/coding_env/lib/python3.7/site-packages/setuptools/_distutils/version.py:346: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  other = LooseVersion(other)\n",
      "/home/masoud/anaconda3/envs/coding_env/lib/python3.7/site-packages/joblib/backports.py:36: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  if LooseVersion(np.__version__) < '1.13':\n",
      "/home/masoud/anaconda3/envs/coding_env/lib/python3.7/site-packages/setuptools/_distutils/version.py:346: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  other = LooseVersion(other)\n",
      "/home/masoud/anaconda3/envs/coding_env/lib/python3.7/site-packages/joblib/backports.py:36: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  if LooseVersion(np.__version__) < '1.13':\n",
      "/home/masoud/anaconda3/envs/coding_env/lib/python3.7/site-packages/setuptools/_distutils/version.py:346: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  other = LooseVersion(other)\n",
      "/home/masoud/anaconda3/envs/coding_env/lib/python3.7/site-packages/joblib/backports.py:36: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  if LooseVersion(np.__version__) < '1.13':\n",
      "/home/masoud/anaconda3/envs/coding_env/lib/python3.7/site-packages/setuptools/_distutils/version.py:346: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  other = LooseVersion(other)\n",
      "/home/masoud/anaconda3/envs/coding_env/lib/python3.7/site-packages/joblib/backports.py:36: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  if LooseVersion(np.__version__) < '1.13':\n",
      "/home/masoud/anaconda3/envs/coding_env/lib/python3.7/site-packages/setuptools/_distutils/version.py:346: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  other = LooseVersion(other)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/masoud/anaconda3/envs/coding_env/lib/python3.7/site-packages/past/builtins/misc.py:45: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
      "  from imp import reload\n",
      "/home/masoud/anaconda3/envs/coding_env/lib/python3.7/site-packages/joblib/backports.py:36: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  if LooseVersion(np.__version__) < '1.13':\n",
      "/home/masoud/anaconda3/envs/coding_env/lib/python3.7/site-packages/setuptools/_distutils/version.py:346: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  other = LooseVersion(other)\n",
      "/home/masoud/anaconda3/envs/coding_env/lib/python3.7/site-packages/joblib/backports.py:36: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  if LooseVersion(np.__version__) < '1.13':\n",
      "/home/masoud/anaconda3/envs/coding_env/lib/python3.7/site-packages/setuptools/_distutils/version.py:346: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  other = LooseVersion(other)\n",
      "/home/masoud/anaconda3/envs/coding_env/lib/python3.7/site-packages/joblib/backports.py:36: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  if LooseVersion(np.__version__) < '1.13':\n",
      "/home/masoud/anaconda3/envs/coding_env/lib/python3.7/site-packages/setuptools/_distutils/version.py:346: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  other = LooseVersion(other)\n",
      "/home/masoud/anaconda3/envs/coding_env/lib/python3.7/site-packages/joblib/backports.py:36: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  if LooseVersion(np.__version__) < '1.13':\n",
      "/home/masoud/anaconda3/envs/coding_env/lib/python3.7/site-packages/setuptools/_distutils/version.py:346: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  other = LooseVersion(other)\n",
      "/home/masoud/anaconda3/envs/coding_env/lib/python3.7/site-packages/joblib/backports.py:36: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  if LooseVersion(np.__version__) < '1.13':\n",
      "/home/masoud/anaconda3/envs/coding_env/lib/python3.7/site-packages/setuptools/_distutils/version.py:346: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  other = LooseVersion(other)\n",
      "/home/masoud/anaconda3/envs/coding_env/lib/python3.7/site-packages/joblib/backports.py:36: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  if LooseVersion(np.__version__) < '1.13':\n",
      "/home/masoud/anaconda3/envs/coding_env/lib/python3.7/site-packages/setuptools/_distutils/version.py:346: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  other = LooseVersion(other)\n",
      "/home/masoud/anaconda3/envs/coding_env/lib/python3.7/site-packages/past/builtins/misc.py:45: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
      "  from imp import reload\n",
      "/home/masoud/anaconda3/envs/coding_env/lib/python3.7/site-packages/joblib/backports.py:36: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  if LooseVersion(np.__version__) < '1.13':\n",
      "/home/masoud/anaconda3/envs/coding_env/lib/python3.7/site-packages/setuptools/_distutils/version.py:346: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  other = LooseVersion(other)\n",
      "/home/masoud/anaconda3/envs/coding_env/lib/python3.7/site-packages/joblib/backports.py:36: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  if LooseVersion(np.__version__) < '1.13':\n",
      "/home/masoud/anaconda3/envs/coding_env/lib/python3.7/site-packages/setuptools/_distutils/version.py:346: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  other = LooseVersion(other)\n",
      "/home/masoud/anaconda3/envs/coding_env/lib/python3.7/site-packages/joblib/backports.py:36: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  if LooseVersion(np.__version__) < '1.13':\n",
      "/home/masoud/anaconda3/envs/coding_env/lib/python3.7/site-packages/setuptools/_distutils/version.py:346: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  other = LooseVersion(other)\n",
      "/home/masoud/anaconda3/envs/coding_env/lib/python3.7/site-packages/joblib/backports.py:36: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  if LooseVersion(np.__version__) < '1.13':\n",
      "/home/masoud/anaconda3/envs/coding_env/lib/python3.7/site-packages/setuptools/_distutils/version.py:346: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  other = LooseVersion(other)\n",
      "/home/masoud/anaconda3/envs/coding_env/lib/python3.7/site-packages/past/builtins/misc.py:45: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
      "  from imp import reload\n",
      "/home/masoud/anaconda3/envs/coding_env/lib/python3.7/site-packages/joblib/backports.py:36: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  if LooseVersion(np.__version__) < '1.13':\n",
      "/home/masoud/anaconda3/envs/coding_env/lib/python3.7/site-packages/setuptools/_distutils/version.py:346: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  other = LooseVersion(other)\n",
      "/home/masoud/anaconda3/envs/coding_env/lib/python3.7/site-packages/joblib/backports.py:36: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  if LooseVersion(np.__version__) < '1.13':\n",
      "/home/masoud/anaconda3/envs/coding_env/lib/python3.7/site-packages/setuptools/_distutils/version.py:346: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  other = LooseVersion(other)\n",
      "/home/masoud/anaconda3/envs/coding_env/lib/python3.7/site-packages/joblib/backports.py:36: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  if LooseVersion(np.__version__) < '1.13':\n",
      "/home/masoud/anaconda3/envs/coding_env/lib/python3.7/site-packages/setuptools/_distutils/version.py:346: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  other = LooseVersion(other)\n",
      "/home/masoud/anaconda3/envs/coding_env/lib/python3.7/site-packages/joblib/backports.py:36: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  if LooseVersion(np.__version__) < '1.13':\n",
      "/home/masoud/anaconda3/envs/coding_env/lib/python3.7/site-packages/setuptools/_distutils/version.py:346: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  other = LooseVersion(other)\n",
      "/home/masoud/anaconda3/envs/coding_env/lib/python3.7/site-packages/joblib/backports.py:36: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  if LooseVersion(np.__version__) < '1.13':\n",
      "/home/masoud/anaconda3/envs/coding_env/lib/python3.7/site-packages/setuptools/_distutils/version.py:346: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  other = LooseVersion(other)\n",
      "/home/masoud/anaconda3/envs/coding_env/lib/python3.7/site-packages/joblib/backports.py:36: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  if LooseVersion(np.__version__) < '1.13':\n",
      "/home/masoud/anaconda3/envs/coding_env/lib/python3.7/site-packages/setuptools/_distutils/version.py:346: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  other = LooseVersion(other)\n",
      "/home/masoud/anaconda3/envs/coding_env/lib/python3.7/site-packages/past/builtins/misc.py:45: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
      "  from imp import reload\n",
      "/home/masoud/anaconda3/envs/coding_env/lib/python3.7/site-packages/joblib/backports.py:36: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  if LooseVersion(np.__version__) < '1.13':\n",
      "/home/masoud/anaconda3/envs/coding_env/lib/python3.7/site-packages/setuptools/_distutils/version.py:346: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  other = LooseVersion(other)\n",
      "/home/masoud/anaconda3/envs/coding_env/lib/python3.7/site-packages/joblib/backports.py:36: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  if LooseVersion(np.__version__) < '1.13':\n",
      "/home/masoud/anaconda3/envs/coding_env/lib/python3.7/site-packages/setuptools/_distutils/version.py:346: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  other = LooseVersion(other)\n",
      "/home/masoud/anaconda3/envs/coding_env/lib/python3.7/site-packages/joblib/backports.py:36: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  if LooseVersion(np.__version__) < '1.13':\n",
      "/home/masoud/anaconda3/envs/coding_env/lib/python3.7/site-packages/setuptools/_distutils/version.py:346: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  other = LooseVersion(other)\n",
      "/home/masoud/anaconda3/envs/coding_env/lib/python3.7/site-packages/joblib/backports.py:36: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  if LooseVersion(np.__version__) < '1.13':\n",
      "/home/masoud/anaconda3/envs/coding_env/lib/python3.7/site-packages/setuptools/_distutils/version.py:346: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  other = LooseVersion(other)\n",
      "/home/masoud/anaconda3/envs/coding_env/lib/python3.7/site-packages/joblib/backports.py:36: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  if LooseVersion(np.__version__) < '1.13':\n",
      "/home/masoud/anaconda3/envs/coding_env/lib/python3.7/site-packages/setuptools/_distutils/version.py:346: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  other = LooseVersion(other)\n",
      "/home/masoud/anaconda3/envs/coding_env/lib/python3.7/site-packages/joblib/backports.py:36: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  if LooseVersion(np.__version__) < '1.13':\n",
      "/home/masoud/anaconda3/envs/coding_env/lib/python3.7/site-packages/setuptools/_distutils/version.py:346: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  other = LooseVersion(other)\n",
      "/home/masoud/anaconda3/envs/coding_env/lib/python3.7/site-packages/joblib/backports.py:36: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  if LooseVersion(np.__version__) < '1.13':\n",
      "/home/masoud/anaconda3/envs/coding_env/lib/python3.7/site-packages/setuptools/_distutils/version.py:346: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  other = LooseVersion(other)\n",
      "/home/masoud/anaconda3/envs/coding_env/lib/python3.7/site-packages/joblib/backports.py:36: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  if LooseVersion(np.__version__) < '1.13':\n",
      "/home/masoud/anaconda3/envs/coding_env/lib/python3.7/site-packages/setuptools/_distutils/version.py:346: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  other = LooseVersion(other)\n"
     ]
    }
   ],
   "source": [
    "pyLDAvis.display(lda_display)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Assign topics to your original data\n",
    "\n",
    "* One practical application of topic modeling is to determine what topic a given text is about\n",
    "* To find that, find the topic number that has the highest percentage contribution in that text\n",
    "* The function, `get_topic_details` shown here, nicely aggregates this information in a presentable table\n",
    "* Combine the original text data with the output of the `get_topic_details` function\n",
    "* Each row contains the dominant topic number, the probability score with that topic and the original text data\n",
    "\n",
    "```python\n",
    "def get_topic_details(ldamodel, corpus):\n",
    "    topic_details_df = pd.DataFrame()\n",
    "    for i, row in enumerate(ldamodel[corpus]):\n",
    "        row = sorted(row, key=lambda x: (x[1]), reverse=True)\n",
    "        for j, (topic_num, prop_topic) in enumerate(row):\n",
    "            if j == 0:  # => dominant topic\n",
    "                wp = ldamodel.show_topic(topic_num)\n",
    "                topic_details_df = topic_details_df.append(pd.Series([topic_num, prop_topic]), ignore_index=True)\n",
    "    topic_details_df.columns = ['Dominant_Topic', '% Score']\n",
    "    return topic_details_df\n",
    "\n",
    "\n",
    "contents = pd.DataFrame({'Original text':text_clean})\n",
    "topic_details = pd.concat([get_topic_details(ldamodel,\n",
    "                           corpus), contents], axis=1)\n",
    "topic_details.head()\n",
    "\n",
    "\n",
    "     Dominant_Topic    % Score     Original text\n",
    "0    0.0              0.989108    [investools, advisory, free, ...\n",
    "1    0.0              0.993513    [forwarded, richard, b, ...\n",
    "2    1.0              0.964858    [hey, wearing, target, purple, ...\n",
    "3    0.0              0.989241    [leslie, milosevich, santa, clara, ...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpreting the topic model\n",
    "\n",
    "* Use the visualization results from the pyLDAvis library\n",
    "* Have a look at topics from the LDA model on the Enron email data. Which one would you research further for fraud detection purposes?\n",
    "\n",
    "**Possible Answers**\n",
    "\n",
    "* __**Topic 4.**__\n",
    "* ~~Topic 3.~~\n",
    "* ~~None of these topics seem related to fraud.~~\n",
    "\n",
    "\n",
    "**Topic 4 seems to discuss the employee share option program, and seems to point to internal conversation (with \"please, may, know\" etc), so this is more likely to be related to the internal accounting fraud and trading stock with insider knowledge. Topic 3 seems to be more related to general news around Enron.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding fraudsters based on topic\n",
    "\n",
    "In this section, you're going to **link the results** from the topic model **back to your original data**. \n",
    "You now learned that you want to **flag everything related to topic 3**. \n",
    "As you will see, this is actually not that straightforward. \n",
    "You'll be given the function `get_topic_details()` which takes the arguments `ldamodel` and `corpus`. \n",
    "It retrieves the details of the topics for each line of text. \n",
    "With that function, you can append the results back to your original data. \n",
    "If you want to learn more detail on how to work with the model results, which is beyond the scope of this notebook, \n",
    "you're highly encouraged to read this [article](https://www.machinelearningplus.com/nlp/topic-modeling-gensim-python/).\n",
    "\n",
    "Available for you are the `dictionary` and `corpus`, the text data `text_clean` as well as your model results `ldamodel`. Also defined is `get_topic_details()`.\n",
    "\n",
    "**Instructions 1/3**\n",
    "\n",
    "* Print and inspect the results from the `get_topic_details()` function by inserting your LDA model results and `corpus`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### def get_topic_details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topic_details(ldamodel, corpus):\n",
    "    topic_details_df = pd.DataFrame()\n",
    "    for i, row in enumerate(ldamodel[corpus]):\n",
    "        row = sorted(row, key=lambda x: (x[1]), reverse=True)\n",
    "        for j, (topic_num, prop_topic) in enumerate(row):\n",
    "            if j == 0:  # => dominant topic\n",
    "                wp = ldamodel.show_topic(topic_num)\n",
    "                topic_details_df = topic_details_df.append(\n",
    "                    pd.Series([topic_num, prop_topic]), ignore_index=True\n",
    "                )\n",
    "    topic_details_df.columns = [\"Dominant_Topic\", \"% Score\"]\n",
    "    return topic_details_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run get_topic_details function and check the results\n",
    "topic_details_df = get_topic_details(ldamodel, corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Dominant_Topic</th>\n",
       "      <th>% Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.0</td>\n",
       "      <td>0.992876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.0</td>\n",
       "      <td>0.800692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.428277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.0</td>\n",
       "      <td>0.993486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.0</td>\n",
       "      <td>0.993389</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Dominant_Topic   % Score\n",
       "0             2.0  0.992876\n",
       "1             2.0  0.800692\n",
       "2             0.0  0.428277\n",
       "3             2.0  0.993486\n",
       "4             2.0  0.993389"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_details_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Dominant_Topic</th>\n",
       "      <th>% Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2085</th>\n",
       "      <td>2.0</td>\n",
       "      <td>0.908144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2086</th>\n",
       "      <td>4.0</td>\n",
       "      <td>0.599751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2087</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.999322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2088</th>\n",
       "      <td>2.0</td>\n",
       "      <td>0.998145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2089</th>\n",
       "      <td>4.0</td>\n",
       "      <td>0.988420</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Dominant_Topic   % Score\n",
       "2085             2.0  0.908144\n",
       "2086             4.0  0.599751\n",
       "2087             1.0  0.999322\n",
       "2088             2.0  0.998145\n",
       "2089             4.0  0.988420"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_details_df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Instructions 2/3**\n",
    "\n",
    "* Concatenate column-wise the results from the previously defined function `get_topic_details()` to the original text data contained under `contents` and inspect the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add original text to topic details in a dataframe\n",
    "contents = pd.DataFrame({\"Original text\": text_clean})\n",
    "topic_details = pd.concat([get_topic_details(ldamodel, corpus), contents], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Dominant_Topic</th>\n",
       "      <th>% Score</th>\n",
       "      <th>Original text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>2.0</td>\n",
       "      <td>0.999957</td>\n",
       "      <td>[joint, venture, enron, meeting, belies, offic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>2.0</td>\n",
       "      <td>0.999953</td>\n",
       "      <td>[lawyer, agree, order, safeguard, document, ho...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>2.0</td>\n",
       "      <td>0.999907</td>\n",
       "      <td>[sample, article, original, message, schmidt, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>849</th>\n",
       "      <td>3.0</td>\n",
       "      <td>0.999874</td>\n",
       "      <td>[original, message, received, thu, aug, cdt, e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>2.0</td>\n",
       "      <td>0.999769</td>\n",
       "      <td>[electricity, trading, build, oh, slowly, fran...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Dominant_Topic   % Score  \\\n",
       "154             2.0  0.999957   \n",
       "135             2.0  0.999953   \n",
       "107             2.0  0.999907   \n",
       "849             3.0  0.999874   \n",
       "149             2.0  0.999769   \n",
       "\n",
       "                                         Original text  \n",
       "154  [joint, venture, enron, meeting, belies, offic...  \n",
       "135  [lawyer, agree, order, safeguard, document, ho...  \n",
       "107  [sample, article, original, message, schmidt, ...  \n",
       "849  [original, message, received, thu, aug, cdt, e...  \n",
       "149  [electricity, trading, build, oh, slowly, fran...  "
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_details.sort_values(by=[\"% Score\"], ascending=False).head(10).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Dominant_Topic</th>\n",
       "      <th>% Score</th>\n",
       "      <th>Original text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>2.0</td>\n",
       "      <td>0.999721</td>\n",
       "      <td>[brazil, scramble, energy, new, york, time, wo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2081</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.999631</td>\n",
       "      <td>[unsubscribe, mailing, please, go, money, net,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>297</th>\n",
       "      <td>2.0</td>\n",
       "      <td>0.999498</td>\n",
       "      <td>[karen, yes, like, sauce, reference, sound, li...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>478</th>\n",
       "      <td>2.0</td>\n",
       "      <td>0.999354</td>\n",
       "      <td>[greeting, jeff, thanks, make, copy, bring, cl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2087</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.999322</td>\n",
       "      <td>[image, image, image, image, image, image, ima...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Dominant_Topic   % Score  \\\n",
       "81               2.0  0.999721   \n",
       "2081             1.0  0.999631   \n",
       "297              2.0  0.999498   \n",
       "478              2.0  0.999354   \n",
       "2087             1.0  0.999322   \n",
       "\n",
       "                                          Original text  \n",
       "81    [brazil, scramble, energy, new, york, time, wo...  \n",
       "2081  [unsubscribe, mailing, please, go, money, net,...  \n",
       "297   [karen, yes, like, sauce, reference, sound, li...  \n",
       "478   [greeting, jeff, thanks, make, copy, bring, cl...  \n",
       "2087  [image, image, image, image, image, image, ima...  "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_details.sort_values(by=[\"% Score\"], ascending=False).head(10).tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Instructions 3/3**\n",
    "\n",
    "* Create a flag with the `np.where()` function to flag all content that has topic 3 as a dominant topic with a 1, and 0 otherwise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create flag for text highest associated with topic 3\n",
    "topic_details[\"flag\"] = np.where((topic_details[\"Dominant_Topic\"] == 3.0), 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_details_1 = topic_details[topic_details.flag == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Dominant_Topic</th>\n",
       "      <th>% Score</th>\n",
       "      <th>Original text</th>\n",
       "      <th>flag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>849</th>\n",
       "      <td>3.0</td>\n",
       "      <td>0.999874</td>\n",
       "      <td>[original, message, received, thu, aug, cdt, e...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014</th>\n",
       "      <td>3.0</td>\n",
       "      <td>0.997888</td>\n",
       "      <td>[forwarded, chris, h, foster, hou, pm, enron, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2039</th>\n",
       "      <td>3.0</td>\n",
       "      <td>0.997664</td>\n",
       "      <td>[w, e, e, k, e, n, e, v, l, b, l, f, r, decemb...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023</th>\n",
       "      <td>3.0</td>\n",
       "      <td>0.997091</td>\n",
       "      <td>[fyi, kim, original, message, schoolcraft, dar...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020</th>\n",
       "      <td>3.0</td>\n",
       "      <td>0.995853</td>\n",
       "      <td>[jerry, remove, distribution, nng, outage, rep...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2054</th>\n",
       "      <td>3.0</td>\n",
       "      <td>0.995691</td>\n",
       "      <td>[sound, like, deny, commodity, logic, please, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>653</th>\n",
       "      <td>3.0</td>\n",
       "      <td>0.994966</td>\n",
       "      <td>[reservation, status, changed, received, see, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2057</th>\n",
       "      <td>3.0</td>\n",
       "      <td>0.994935</td>\n",
       "      <td>[reservation, status, changed, detail, reserva...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2031</th>\n",
       "      <td>3.0</td>\n",
       "      <td>0.993948</td>\n",
       "      <td>[forwarded, eric, boyt, corp, enron, pm, jason...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015</th>\n",
       "      <td>3.0</td>\n",
       "      <td>0.993089</td>\n",
       "      <td>[sound, good, let, know, time, gonna, work, or...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Dominant_Topic   % Score  \\\n",
       "849              3.0  0.999874   \n",
       "2014             3.0  0.997888   \n",
       "2039             3.0  0.997664   \n",
       "2023             3.0  0.997091   \n",
       "2020             3.0  0.995853   \n",
       "2054             3.0  0.995691   \n",
       "653              3.0  0.994966   \n",
       "2057             3.0  0.994935   \n",
       "2031             3.0  0.993948   \n",
       "2015             3.0  0.993089   \n",
       "\n",
       "                                          Original text  flag  \n",
       "849   [original, message, received, thu, aug, cdt, e...     1  \n",
       "2014  [forwarded, chris, h, foster, hou, pm, enron, ...     1  \n",
       "2039  [w, e, e, k, e, n, e, v, l, b, l, f, r, decemb...     1  \n",
       "2023  [fyi, kim, original, message, schoolcraft, dar...     1  \n",
       "2020  [jerry, remove, distribution, nng, outage, rep...     1  \n",
       "2054  [sound, like, deny, commodity, logic, please, ...     1  \n",
       "653   [reservation, status, changed, received, see, ...     1  \n",
       "2057  [reservation, status, changed, detail, reserva...     1  \n",
       "2031  [forwarded, eric, boyt, corp, enron, pm, jason...     1  \n",
       "2015  [sound, good, let, know, time, gonna, work, or...     1  "
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_details_1.sort_values(by=[\"% Score\"], ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**You have now flagged all data that is highest associated with topic 3, that seems to cover internal conversation about enron stock options. You are a true detective. With these sections you have demonstrated that text mining and topic modeling can be a powerful tool for fraud detection.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text mining for fraud detection\n",
    "\n",
    "* Know how to augment fraud detection analysis with text mining techniques\n",
    "* Applied word searches to flag use of certain words, and learned how to apply topic modeling for fraud detection\n",
    "* Learned how to effectively clean messy text data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further learning for fraud detection\n",
    "\n",
    "* Network analysis to detect fraud\n",
    "* Different supervised and unsupervised learning techniques (e.g. Neural Networks)\n",
    "* Working with very large data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
